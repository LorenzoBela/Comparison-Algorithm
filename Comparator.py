"""
This script was developed by Lorenzo Bela under the supervision of the Data Warehouse and Analytics Division.

It is intended for use in the HRMIS JoPayroll Backlog process, specifically for comparing the consolidated payroll master list with the report generated by the system.

Please follow the instructions carefully and make any necessary code modifications as needed.

Confidential information must not be shared outside the division, and this script must be used strictly for its intended purposes.

Thank you. :>
"""
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
import re
import csv
from datetime import datetime
import subprocess
import sys
from collections import defaultdict
from functools import lru_cache
import bisect

warnings.filterwarnings('ignore')

# Optional third-party enhancements (loaded lazily, fallback-safe)
try:
    import ftfy as _ftfy
except Exception:
    _ftfy = None
try:
    from unidecode import unidecode as _unidecode
except Exception:
    _unidecode = None
try:
    from diskcache import Cache as _DiskCache
except Exception:
    _DiskCache = None
try:
    import textdistance as _textdistance
except Exception:
    _textdistance = None
try:
    from abydos.phonetic import DaitchMokotoff as _DM, FuzzySoundex as _FS
except Exception:
    _DM = None
    _FS = None
try:
    from nameparser import HumanName as _HumanName
except Exception:
    _HumanName = None
try:
    from stdnum.ph import tin as _ph_tin
except Exception:
    _ph_tin = None
try:
    from joblib import Parallel as _Parallel, delayed as _delayed
except Exception:
    _Parallel = None
    _delayed = None
try:
    from tqdm import tqdm as _tqdm
except Exception:
    _tqdm = None
try:
    import faiss as _faiss
except Exception:
    _faiss = None
try:
    import spacy as _spacy
    _nlp = _spacy.blank("en")
except Exception:
    _spacy = None
    _nlp = None
try:
    from huggingface_hub import snapshot_download as _hf_snapshot
except Exception:
    _hf_snapshot = None

# AI Model - will be loaded once and reused
_ai_model = None
_embedding_cache = {}
_pair_ai_cache = {}

# Optional disk-backed cache for embeddings/similarities
_disk_cache = None
if _DiskCache is not None:
    try:
        _disk_cache = _DiskCache(".ai_cache")
    except Exception:
        _disk_cache = None

# Optional FAISS index for dataset2 names
_faiss_name_index = None
_faiss_name_ids = None

# Common name suffixes and particles
_NAME_SUFFIXES = {"JR", "SR", "JR.", "SR.", "II", "III", "IV", "V"}
_SURNAME_PARTICLES = {"DE", "DEL", "DELA", "DELOS", "DE LOS", "DE LAS", "DI", "DA", "DOS"}

# Tunable knobs for candidate sizes
_FAISS_K = 120
_PRUNE_TOPN_FAISS = 80
_PRUNE_TOPN_SURNAME = 120

# Parallel execution knobs (requires joblib if available)
_PARALLEL_ENABLED = True
_PARALLEL_N_JOBS = -1  # use all cores
_PARALLEL_BACKEND = "threading"  # valid joblib backend name

def ensure_optional_libs():
    """Install optional libraries (rapidfuzz, jellyfish) if missing."""
    try:
        import rapidfuzz  # noqa: F401
    except Exception:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "rapidfuzz"])
        except Exception:
            pass
    try:
        import jellyfish  # noqa: F401
    except Exception:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "jellyfish"])
        except Exception:
            pass

# Try to ensure optional libs at import time (best-effort)
ensure_optional_libs()

def ensure_sentence_transformers():
    """Ensure sentence-transformers is installed"""
    try:
        import sentence_transformers
        return True
    except ImportError:
        print("ðŸ”„ Installing sentence-transformers library...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers"])
            print("âœ… sentence-transformers installed successfully!")
            return True
        except Exception as e:
            print(f"âŒ Failed to install sentence-transformers: {e}")
            return False

def load_ai_model():
    """Load the sentence-transformers model once and reuse it"""
    global _ai_model
    if _ai_model is None:
        if not ensure_sentence_transformers():
            return None
        
        try:
            from sentence_transformers import SentenceTransformer
            model_id = 'sentence-transformers/all-MiniLM-L6-v2'
            print("ðŸ¤– Loading AI model 'all-MiniLM-L6-v2'...")
            # If huggingface-hub is available, resolve to a local snapshot for stability
            if _hf_snapshot is not None:
                try:
                    local_dir = _hf_snapshot(repo_id=model_id)
                    _ai_model = SentenceTransformer(local_dir)
                except Exception:
                    _ai_model = SentenceTransformer('all-MiniLM-L6-v2')
            else:
                _ai_model = SentenceTransformer('all-MiniLM-L6-v2')
            print("âœ… AI model loaded successfully!")
        except Exception as e:
            print(f"âŒ Failed to load AI model: {e}")
            _ai_model = None
    
    return _ai_model

def _normalize_vecs_for_cosine(np_array: np.ndarray) -> np.ndarray:
    """L2-normalize embeddings for cosine similarity with FAISS inner product."""
    if np_array is None or len(np_array) == 0:
        return np_array
    norms = np.linalg.norm(np_array, axis=1, keepdims=True) + 1e-12
    return np_array / norms

def build_faiss_name_index(dataset2) -> None:
    """Build a FAISS index over dataset2 Name embeddings for fast candidate search."""
    global _faiss_name_index, _faiss_name_ids
    if _faiss is None:
        return
    model = load_ai_model()
    if model is None:
        return
    try:
        names = [str(n) if not pd.isna(n) else '' for n in list(dataset2['Name'])]
        embs = model.encode(names, batch_size=128, show_progress_bar=False)
        embs = np.array(embs, dtype='float32')
        embs = _normalize_vecs_for_cosine(embs)
        dim = embs.shape[1]
        index = _faiss.IndexFlatIP(dim)
        index.add(embs)
        _faiss_name_index = index
        _faiss_name_ids = np.array(list(dataset2.index))
    except Exception:
        _faiss_name_index = None
        _faiss_name_ids = None

def faiss_topk_candidates(name: str, k: int = 50) -> list:
    """Return list of dataset2 indices for top-k nearest names via FAISS, or empty on failure."""
    if _faiss_name_index is None:
        return []
    model = load_ai_model()
    if model is None:
        return []
    try:
        emb = model.encode([str(name)], show_progress_bar=False)
        emb = np.array(emb, dtype='float32')
        emb = _normalize_vecs_for_cosine(emb)
        D, I = _faiss_name_index.search(emb, min(k, _faiss_name_index.ntotal))
        idxs = I[0].tolist()
        return [int(_faiss_name_ids[i]) for i in idxs if i >= 0]
    except Exception:
        return []

def calculate_semantic_similarity(text1, text2):
    """Calculate semantic similarity using cached embeddings for speed/stability."""
    model = load_ai_model()
    if model is None:
        return 0.0

    try:
        from sentence_transformers.util import cos_sim

        def _get_embedding(text: str):
            key = ("emb", text)
            # In-memory cache first
            if key in _embedding_cache:
                return _embedding_cache[key]
            # On-disk cache next
            if _disk_cache is not None:
                try:
                    disk_key = f"emb::{text}"
                    if disk_key in _disk_cache:
                        emb = _disk_cache[disk_key]
                        _embedding_cache[key] = emb
                        return emb
                except Exception:
                    pass
            # Compute and cache
            emb = model.encode([text])[0]
            _embedding_cache[key] = emb
            if _disk_cache is not None:
                try:
                    disk_key = f"emb::{text}"
                    _disk_cache[disk_key] = emb
                except Exception:
                    pass
            return emb

        # Clean and normalize texts
        t1 = str(text1).strip() if text1 else ""
        t2 = str(text2).strip() if text2 else ""
        if not t1 or not t2:
            return 0.0

        v1 = _get_embedding(t1)
        v2 = _get_embedding(t2)
        similarity = cos_sim(v1, v2).item()
        return max(0.0, min(1.0, float(similarity)))

    except Exception as e:
        print(f"âš ï¸ Error calculating semantic similarity: {e}")
        return 0.0

# ----------------------------
# Cached string processing utils
# ----------------------------

@lru_cache(maxsize=200_000)
def _clean_name_for_matching_cached(name_str: str) -> str:
    name = str(name_str).strip()
    name = ' '.join(name.split())
    try:
        if _ftfy is not None:
            name = _ftfy.fix_text(name)
    except Exception:
        pass
    try:
        if _unidecode is not None:
            name = _unidecode(name)
    except Exception:
        pass
    name = re.sub(r'[^\w\s]', ' ', name)
    name = ' '.join(name.split())
    name = name.upper()
    return name


@lru_cache(maxsize=200_000)
def _extract_name_parts_cached(name_str: str):
    name = str(name_str).strip()
    name = ' '.join(name.split())
    if _HumanName is not None:
        try:
            parsed = _HumanName(name)
            last_name = (parsed.last or parsed.suffix or parsed.title or '').strip() or parsed.last
            last_name = parsed.last.strip() if parsed.last else (name.split(',')[0].strip() if ',' in name else name.split(' ')[0])
            first_name = parsed.first.strip() if parsed.first else ''
            middle_part = parsed.middle.strip() if parsed.middle else ''
            return last_name.title().strip(), first_name.title().strip(), middle_part.title().strip()
        except Exception:
            pass
    if ',' in name:
        parts = name.split(',', 1)
        last_name = parts[0].strip()
        first_part = parts[1].strip() if len(parts) > 1 else ""
    else:
        parts = name.split()
        if len(parts) >= 2:
            last_name = parts[0]
            first_part = ' '.join(parts[1:])
        else:
            last_name = name
            first_part = ""
    first_part = ' '.join(first_part.split())
    first_name = first_part.split()[0] if first_part.split() else ""
    middle_part = ' '.join(first_part.split()[1:]) if len(first_part.split()) > 1 else ""
    last_name = last_name.title().strip()
    first_name = first_name.title().strip()
    middle_part = middle_part.title().strip()
    return last_name, first_name, middle_part


@lru_cache(maxsize=200_000)
def _normalize_position_cached(pos_str: str) -> str:
    pos = str(pos_str).strip().upper()
    if _nlp is not None:
        try:
            doc = _nlp(pos)
            pos = ' '.join([t.text for t in doc]).upper()
        except Exception:
            pass
    pos = re.sub(r'[^\w\s]', ' ', pos)
    pos = ' '.join(pos.split())
    position_abbreviations = {
        'ADAS5': 'ADMINISTRATIVE ASSISTANT V',
        'ADAS4': 'ADMINISTRATIVE ASSISTANT IV',
        'ADAS3': 'ADMINISTRATIVE ASSISTANT III',
        'ADAS2': 'ADMINISTRATIVE ASSISTANT II',
        'ADAS1': 'ADMINISTRATIVE ASSISTANT I',
        'ADAS': 'ADMINISTRATIVE ASSISTANT',
        'ADMIN5': 'ADMINISTRATIVE OFFICER V',
        'ADMIN4': 'ADMINISTRATIVE OFFICER IV',
        'ADMIN3': 'ADMINISTRATIVE OFFICER III',
        'ADMIN2': 'ADMINISTRATIVE OFFICER II',
        'ADMIN1': 'ADMINISTRATIVE OFFICER I',
        'ADMIN': 'ADMINISTRATIVE OFFICER',
        'ENG5': 'ENGINEER V',
        'ENG4': 'ENGINEER IV',
        'ENG3': 'ENGINEER III',
        'ENG2': 'ENGINEER II',
        'ENG1': 'ENGINEER I',
        'ENG': 'ENGINEER',
        'ARCH5': 'ARCHITECT V',
        'ARCH4': 'ARCHITECT IV',
        'ARCH3': 'ARCHITECT III',
        'ARCH2': 'ARCHITECT II',
        'ARCH1': 'ARCHITECT I',
        'ARCH': 'ARCHITECT',
        'SPEC5': 'SPECIALIST V',
        'SPEC4': 'SPECIALIST IV',
        'SPEC3': 'SPECIALIST III',
        'SPEC2': 'SPECIALIST II',
        'SPEC1': 'SPECIALIST I',
        'SPEC': 'SPECIALIST',
        'OFF5': 'OFFICER V',
        'OFF4': 'OFFICER IV',
        'OFF3': 'OFFICER III',
        'OFF2': 'OFFICER II',
        'OFF1': 'OFFICER I',
        'OFF': 'OFFICER'
    }
    for abbrev, full_name in position_abbreviations.items():
        if abbrev in pos:
            pos = pos.replace(abbrev, full_name)
    extra_phrases = [
        'AIR-CONDITIONING TECHNICIAN',
        'COMPUTER OPERATOR',
        'ELECTRONICS AND COMMUNICATIONS EQUIPMENT TECHNICIAN',
        'ECET',
        'PRIVATE SECRETARY',
        'DRIVER',
        'COURRIER',
        'CHAUFFEUR',
        'WATCHMAN',
        'UTILITY WORKER',
        'CARPENTER FOREMAN',
        'CONSTRUCTION AND MAINTENANCE GENERAL FOREMAN',
        'N/A',
        'BIGLANG-AWA',
        'ADLAON',
        'PUNTIL',
        'BARCELLANO',
        'DE GUZMAN',
        'ARANETA',
        'TINIO',
        'ANDAYA',
        'MACEREN',
        'GONZALES',
        'BOLOR',
        'ANTONIO',
        'DUENAS',
        'ALIASOT',
        'MENDOZA',
        'GAJO',
        'BABLES',
        'JUNIO',
        'SAMANTE',
        'BARLAN',
        'FORTALEZA',
        'GALANG',
        'ALBERT',
        'SANTIAGO',
        'DE GUZMAN',
        'LORO',
        'CASTRO',
        'SAN DIEGO',
        'FLORES',
        'MARTIN',
        'ENTION',
        'PAET',
        'LLAMAS'
    ]
    for phrase in extra_phrases:
        pos = pos.replace(phrase, '').strip()
    position_mappings = {
        'ADMINISTRATIVE AIDE': 'ADMINISTRATIVE AIDE',
        'ADMINISTRATIVE ASSISTANT': 'ADMINISTRATIVE ASSISTANT',
        'ADMINISTRATIVE OFFICER': 'ADMINISTRATIVE OFFICER',
        'ADMINISTRATIVE OFICER': 'ADMINISTRATIVE OFFICER',
        'SENIOR ADMINISTRATIVE ASISSTANT': 'SENIOR ADMINISTRATIVE ASSISTANT',
        'PROJECT DEVLEOPMENT OFFICER': 'PROJECT DEVELOPMENT OFFICER',
        'PERSONNEL SPECIALIST 1': 'PERSONNEL SPECIALIST I',
        'INFORMATION SYSTEMS ANALYST': 'INFORMATION SYSTEMS ANALYST',
        'INFORMATION SYSTEM ANALYST': 'INFORMATION SYSTEMS ANALYST',
        'PROCUREMENT MANAGEMENT OFFICER': 'PROCUREMENT MANAGEMENT OFFICER',
        'DRIVER-COURRIER': 'DRIVER-COURIER',
        'DRIVER COURIER': 'DRIVER-COURIER',
        'DRIVER-COURRIER': 'DRIVER-COURIER',
        'PLANNING OFFICER': 'PLANNING OFFICER',
        'PROJECT DEVELOPMENT OFFICER': 'PROJECT DEVELOPMENT OFFICER',
        'COMPUTER MAINTENANCE TECHNOLOGIST': 'COMPUTER MAINTENANCE TECHNOLOGIST',
        'INFORMATION OFFICER': 'INFORMATION OFFICER',
        'HUMAN RESOURCE MANAGEMENT OFFICER': 'HUMAN RESOURCE MANAGEMENT OFFICER',
        'COMPUTER PROGRAMMER': 'COMPUTER PROGRAMMER',
        'INFORMATION SYSTEMS RESEARCHER': 'INFORMATION SYSTEMS RESEARCHER',
        'INFORMATION SYSTEMS ANALYST': 'INFORMATION SYSTEMS ANALYST',
        'INFORMATION TECHNOLOGY OFFICER': 'INFORMATION TECHNOLOGY OFFICER',
        'DEVELOPMENT MANAGEMENT OFFICER': 'DEVELOPMENT MANAGEMENT OFFICER',
        'CREATIVE ARTS SPECIALIST': 'CREATIVE ARTS SPECIALIST',
        'TRAINING SPECIALIST': 'TRAINING SPECIALIST',
        'ACCOUNTING ANALYST': 'ACCOUNTING ANALYST',
        'INTERNAL AUDITOR': 'INTERNAL AUDITOR',
        'SECURITY GUARD': 'SECURITY GUARD',
        'ARCHITECT': 'ARCHITECT',
        'ENGINEER': 'ENGINEER',
        'MEDICAL OFFICER': 'MEDICAL OFFICER',
        'SUPERVISING ADMINISTRATIVE OFFICER': 'SUPERVISING ADMINISTRATIVE OFFICER',
        'EXECUTIVE ASSISTANT': 'EXECUTIVE ASSISTANT'
    }
    for old_pos, new_pos in position_mappings.items():
        if old_pos in pos:
            pos = pos.replace(old_pos, new_pos)
    pos = ' '.join(pos.split())
    return pos


def _normalize_tin_value(tin: str) -> str:
    tin_str = str(tin).strip()
    try:
        if _ph_tin is not None:
            compact = _ph_tin.compact(tin_str)
            return compact if _ph_tin.is_valid(compact) else tin_str.replace('-', '').replace(' ', '')
    except Exception:
        pass
    return tin_str.replace('-', '').replace(' ', '')
def double_metaphone_lastname(name: str):
    """Return simple phonetic encodings for last name (fallback-safe)."""
    try:
        import jellyfish
    except Exception:
        return ("", "")
    last, _, _ = extract_name_parts_enhanced(name)
    if not last:
        return ("", "")
    # Use available phonetic keys (metaphone/nysiis) as proxies
    try:
        m1 = jellyfish.metaphone(last)
    except Exception:
        m1 = ""
    try:
        m2 = jellyfish.nysiis(last)
    except Exception:
        m2 = ""
    # If abydos available, add stronger phonetic keys and merge
    try:
        dm = _DM() if _DM is not None else None
        fs = _FS() if _FS is not None else None
        m3 = dm.encode(last) if dm is not None else ""
        m4 = fs.encode(last) if fs is not None else ""
    except Exception:
        m3 = ""
        m4 = ""
    return (m1 or m3 or "", m2 or m4 or "")

def _strip_suffix(token: str) -> str:
    t = token.strip().strip("., ")
    return "" if t.upper() in _NAME_SUFFIXES else token

def _normalize_tokens(text: str) -> list:
    if not text:
        return []
    # Replace hyphens with spaces, collapse whitespace
    t = re.sub(r"[-]+", " ", str(text))
    t = ' '.join(t.split()).strip()
    if not t:
        return []
    return [tok for tok in t.split(' ') if tok]

@lru_cache(maxsize=400_000)
def _surname_candidates_cached(name: str) -> tuple:
    last, first, middle = extract_name_parts_enhanced(name)
    cand = set()
    for tok in _normalize_tokens(last):
        tok_clean = _strip_suffix(tok)
        if tok_clean:
            cand.add(tok_clean.upper())
    fm = ' '.join([p for p in [first, middle] if p]).strip()
    fm_tokens = _normalize_tokens(fm)
    if fm_tokens:
        last_tok = fm_tokens[-1].upper()
        if last_tok not in _NAME_SUFFIXES:
            cand.add(last_tok)
        if len(fm_tokens) >= 2:
            part = fm_tokens[-2].upper()
            if part in _SURNAME_PARTICLES:
                cand.add((part + " " + fm_tokens[-1]).upper())
                cand.add(fm_tokens[-1].upper())
    return tuple(sorted(c for c in cand if c))


def surname_candidates(name: str) -> set:
    """Cached set of candidate surname tokens from a person name."""
    try:
        return set(_surname_candidates_cached(str(name)))
    except Exception:
        return set()

@lru_cache(maxsize=500_000)
def _pair_ai_scores_cached(name1: str, name2: str, pos1: str, pos2: str):
    n_sim = calculate_semantic_similarity(name1, name2)
    p_sim = calculate_semantic_similarity(pos1, pos2)
    pos_rule_sim = are_positions_similar(pos1, pos2)
    return (n_sim, p_sim, pos_rule_sim)


def get_pair_ai_scores(row1, row2):
    """Compute/cached AI semantic scores for name and position for a pair."""
    key = (
        str(row1.get('Name', '')).strip(),
        str(row2.get('Name', '')).strip(),
        str(row1.get('Position', '')).strip(),
        str(row2.get('Position', '')).strip(),
    )
    if key in _pair_ai_cache:
        return _pair_ai_cache[key]
    scores = _pair_ai_scores_cached(*key)
    _pair_ai_cache[key] = scores
    return scores

def ai_confidence_boost(row1, row2, base_conf: float, min_name_gate: float = 0.0) -> tuple:
    """Blend AI name/position similarity into a base confidence.
    Returns (adjusted_confidence, ai_name, ai_pos, pos_rule_sim).
    """
    ai_name, ai_pos, pos_rule_sim = get_pair_ai_scores(row1, row2)
    # Start from the provided base confidence
    adj = float(base_conf)
    # If a minimum AI name similarity is required, enforce it softly
    if min_name_gate and ai_name < min_name_gate:
        # Penalize base if AI disagrees strongly
        adj = max(0.0, adj - 0.25)
    # Blend: heavier on AI name, lighter on position
    adj = 0.6 * adj + 0.3 * ai_name + 0.1 * max(ai_pos, 0.8 if pos_rule_sim else 0.0)
    return (max(0.0, min(1.0, adj)), ai_name, ai_pos, pos_rule_sim)

def advanced_name_similarity(name1: str, name2: str) -> float:
    """Blend RapidFuzz measures for robust name similarity. Returns 0..1. (cached)"""
    try:
        from rapidfuzz import fuzz
    except Exception:
        # Fallback to existing similarity
        return max(0.0, min(1.0, float(calculate_name_similarity(name1, name2))))

    n1 = clean_name_for_matching(name1)
    n2 = clean_name_for_matching(name2)
    if not n1 or not n2:
        return 0.0

    ts = fuzz.token_set_ratio(n1, n2) / 100.0
    pr = fuzz.partial_ratio(n1, n2) / 100.0
    wr = fuzz.WRatio(n1, n2) / 100.0
    base = max(ts, (ts + pr) / 2.0, wr)
    
    # Optional extra signal: Jaro-Winkler from textdistance/abydos
    try:
        if _textdistance is not None:
            jw = float(_textdistance.jaro_winkler(n1, n2))
            base = max(base, jw)
    except Exception:
        pass

    l1, f1, _ = extract_name_parts_enhanced(name1)
    l2, f2, _ = extract_name_parts_enhanced(name2)
    if l1 and l2 and l1.upper() == l2.upper() and f1 and f2 and f1[0].upper() == f2[0].upper():
        base = min(1.0, base + 0.05)
    return float(base)

def names_compatible_strict(name1: str, name2: str) -> bool:
    """Hard gate: surnames must intersect (or be phonetically close) + minimum similarity (cached)."""
    s1 = surname_candidates(name1)
    s2 = surname_candidates(name2)
    direct_hit = bool(s1 & s2)
    if not direct_hit:
        m1a, m1b = double_metaphone_lastname(name1)
        m2a, m2b = double_metaphone_lastname(name2)
        phonetic_close = (m1a and (m1a == m2a or m1a == m2b)) or (m1b and (m1b == m2a or m1b == m2b))
        if not phonetic_close:
            return False
    sim = advanced_name_similarity(name1, name2)
    if sim >= 0.50:
        return True
    ai_name = calculate_semantic_similarity(name1, name2)
    return ai_name >= 0.70

def count_rule_based_passes(row1, row2):
    """Count how many rule-based checks a record pair passes (out of 10)"""
    passes = 0
    
    # Rule 1: Exact name match
    if str(row1['Name']).strip().upper() == str(row2['Name']).strip().upper():
        passes += 1
    
    # Rule 2: First and last name component match
    last1, first1, _ = extract_name_parts_enhanced(row1['Name'])
    last2, first2, _ = extract_name_parts_enhanced(row2['Name'])
    if (last1.upper() == last2.upper() and first1.upper() == first2.upper() and
        last1 and first1 and last2 and first2):
        passes += 1
    
    # Rule 3: TIN exact match (normalized + validated if possible)
    tin1 = str(row1['TIN']).strip()
    tin2 = str(row2['TIN']).strip()
    # Use python-stdnum if available for normalization
    try:
        if _ph_tin is not None:
            tin1_norm_v = _ph_tin.compact(tin1)
            tin2_norm_v = _ph_tin.compact(tin2)
            # If valid, use compact form; else keep original
            tin1 = tin1_norm_v if _ph_tin.is_valid(tin1_norm_v) else tin1
            tin2 = tin2_norm_v if _ph_tin.is_valid(tin2_norm_v) else tin2
    except Exception:
        pass
    if tin1 and tin2 and tin1 != 'nan' and tin2 != 'nan' and tin1 == tin2:
        passes += 1
    
    # Rule 4: TIN format-normalized match
    tin1_norm = tin1.replace('-', '').replace(' ', '') if tin1 and tin1 != 'nan' else ""
    tin2_norm = tin2.replace('-', '').replace(' ', '') if tin2 and tin2 != 'nan' else ""
    if tin1_norm and tin2_norm and len(tin1_norm) >= 9 and tin1_norm == tin2_norm:
        passes += 1
    
    # Rule 5: Net pay exact match
    pay1 = row1['Pay']
    pay2 = row2['Pay']
    if not pd.isna(pay1) and not pd.isna(pay2) and abs(pay1 - pay2) < 0.01:
        passes += 1
    
    # Rule 6: Net pay close match (within 5%)
    if not pd.isna(pay1) and not pd.isna(pay2) and pay1 > 0 and pay2 > 0:
        diff_pct = abs(pay1 - pay2) / max(pay1, pay2)
        if diff_pct <= 0.05:
            passes += 1
    
    # Rule 7: Position exact match
    pos1 = str(row1['Position']).strip().upper()
    pos2 = str(row2['Position']).strip().upper()
    if pos1 and pos2 and pos1 != 'NAN' and pos2 != 'NAN' and pos1 == pos2:
        passes += 1
    
    # Rule 8: Position normalized match
    pos1_norm = normalize_position(row1['Position'])
    pos2_norm = normalize_position(row2['Position'])
    if pos1_norm and pos2_norm and pos1_norm == pos2_norm:
        passes += 1
    
    # Rule 9: Fuzzy name match (high similarity)
    name_similarity = calculate_name_similarity(row1['Name'], row2['Name'])
    if name_similarity >= 0.8:
        passes += 1
    
    # Rule 10: Comprehensive score match
    comprehensive_score = calculate_comprehensive_match_score(row1, row2)
    if comprehensive_score >= 0.7:
        passes += 1
    
    return passes

def ai_enhanced_match_decision(row1, row2):
    """
    Make match decision using rule-based + AI approach:
    - â‰¥9 passes: Automatic MATCH
    - â‰¤5 passes: Automatic MISMATCH  
    - 6-8 passes: Use AI semantic similarity with intelligent thresholds
    """
    rule_passes = count_rule_based_passes(row1, row2)
    
    # CRITICAL: First check if names are fundamentally different people
    # This prevents matching completely different people based on position alone
    name1 = str(row1['Name']).strip()
    name2 = str(row2['Name']).strip()
    
    # Strong name gate unless TINs are exact
    tin1 = str(row1['TIN']).strip() if 'TIN' in row1 else ''
    tin2 = str(row2['TIN']).strip() if 'TIN' in row2 else ''
    tin1n = tin1.replace('-', '').replace(' ', '') if tin1 else ''
    tin2n = tin2.replace('-', '').replace(' ', '') if tin2 else ''
    tin_strong = bool(tin1n and tin2n and tin1n == tin2n)

    if not tin_strong and not names_compatible_strict(name1, name2):
        return {
            'decision': 'MISMATCH (NAME_GATE)',
            'rule_passes': rule_passes,
            'ai_used': False,
            'name_semantic_score': 0.0,
            'position_semantic_score': 0.0,
            'confidence': 0.98,
            'explanation': "Rejected by strict name gate (last/phonetic last mismatch or low similarity)"
        }
    
    # Automatic decisions for clear cases
    if rule_passes >= 9:
        return {
            'decision': 'MATCH',
            'rule_passes': rule_passes,
            'ai_used': False,
            'name_semantic_score': None,
            'position_semantic_score': None,
            'confidence': 0.95 + (rule_passes - 9) * 0.05  # High confidence for 9-10 passes
        }
    
    if rule_passes <= 5:
        return {
            'decision': 'MISMATCH',
            'rule_passes': rule_passes,
            'ai_used': False,
            'name_semantic_score': None,
            'position_semantic_score': None,
            'confidence': 0.95 - (rule_passes / 5) * 0.2  # High confidence for low passes
        }
    
    # Borderline cases (6-8 passes): Use AI with intelligent thresholds
    name_semantic_score = calculate_semantic_similarity(row1['Name'], row2['Name'])
    position_semantic_score = calculate_semantic_similarity(row1['Position'], row2['Position'])
    
    # Enhanced AI decision logic with multiple criteria
    name_threshold = 0.80  # Slightly more lenient for names
    position_threshold = 0.75  # More lenient for positions (handles abbreviations better)
    
    # Additional checks for position similarity
    pos1_normalized = normalize_position(row1['Position'])
    pos2_normalized = normalize_position(row2['Position'])
    positions_are_similar = are_positions_similar(row1['Position'], row2['Position'])
    
    # AI decision with fallback logic
    name_passes_ai = name_semantic_score >= name_threshold
    position_passes_ai = (position_semantic_score >= position_threshold or positions_are_similar)
    
    # Decision matrix:
    # 1. Both name and position pass AI thresholds â†’ MATCH
    # 2. Name passes AND positions are rule-based similar â†’ MATCH  
    # 3. High rule passes (7-8) AND name passes â†’ MATCH (benefit of doubt)
    # 4. Otherwise â†’ MISMATCH
    
    if name_passes_ai and position_passes_ai:
        decision = 'MATCH (AI)'
        confidence = (name_semantic_score + max(position_semantic_score, 0.85 if positions_are_similar else 0)) / 2
    elif name_passes_ai and positions_are_similar:
        decision = 'MATCH (AI-HYBRID)'  # AI name + rule-based position
        confidence = (name_semantic_score + 0.80) / 2  # Give rule-based position match a score
    elif rule_passes >= 7 and name_passes_ai:
        decision = 'MATCH (AI-ASSISTED)'  # High rule confidence + AI name confirmation
        confidence = (name_semantic_score + 0.75) / 2
    else:
        decision = 'MISMATCH (AI)'
        confidence = 1.0 - ((name_semantic_score + position_semantic_score) / 2)
    
    return {
        'decision': decision,
        'rule_passes': rule_passes,
        'ai_used': True,
        'name_semantic_score': name_semantic_score,
        'position_semantic_score': position_semantic_score,
        'confidence': confidence,
        'position_rule_similar': positions_are_similar,
        'explanation': f"Name: {name_semantic_score:.3f} (â‰¥{name_threshold}), Position: {position_semantic_score:.3f} (â‰¥{position_threshold}), Rule-similar: {positions_are_similar}"
    }

def get_user_columns():
    """Get column ranges from user input"""
    print("\n" + "="*60)
    print("MANUAL COLUMN RANGE INPUT")
    print("="*60)
    
    print("\nðŸ“‹ DATASET 1 COLUMN RANGES:")
    print("Enter the column ranges for the first dataset (e.g., A:D)")
    
    # Get Dataset 1 column ranges
    dataset1_range = input("Dataset 1 range (e.g., A:D): ").strip()
    
    print("\nðŸ“‹ DATASET 2 COLUMN RANGES:")
    print("Enter the column ranges for the second dataset (e.g., F:I)")
    
    # Get Dataset 2 column ranges
    dataset2_range = input("Dataset 2 range (e.g., F:I): ").strip()
    
    # Parse column ranges
    coords = {
        'dataset1': parse_column_range(dataset1_range),
        'dataset2': parse_column_range(dataset2_range)
    }
    
    print(f"\nâœ… Column ranges set:")
    print(f"Dataset 1: {dataset1_range}")
    print(f"Dataset 2: {dataset2_range}")
    
    return coords

def parse_column_range(range_str):
    """Parse column range like 'A:D' into column info"""
    try:
        # Remove any spaces
        range_str = range_str.replace(' ', '')
        
        # Split by colon
        if ':' in range_str:
            start_col, end_col = range_str.split(':')
        else:
            start_col = end_col = range_str
        
        # Parse start column
        start_col_idx = parse_column_letter(start_col)
        
        # Parse end column
        end_col_idx = parse_column_letter(end_col)
        
        return {
            'start_col': start_col_idx,
            'end_col': end_col_idx,
            'range_str': range_str
        }
    except Exception as e:
        print(f"Error parsing column range '{range_str}': {e}")
        return None

def parse_excel_range(range_str):
    """Parse Excel range like 'A1:A421' into column and row info"""
    try:
        # Remove any spaces
        range_str = range_str.replace(' ', '')
        
        # Split by colon
        if ':' in range_str:
            start, end = range_str.split(':')
        else:
            start = end = range_str
        
        # Parse start coordinates
        start_col, start_row = parse_cell_reference(start)
        
        # Parse end coordinates
        end_col, end_row = parse_cell_reference(end)
        
        return {
            'start_col': start_col,
            'start_row': start_row,
            'end_col': end_col,
            'end_row': end_row,
            'range_str': range_str
        }
    except Exception as e:
        print(f"Error parsing range '{range_str}': {e}")
        return None

def parse_column_letter(col_letters):
    """Parse column letters like 'A' or 'AA' into column index"""
    # Convert column letters to index (A=0, B=1, etc.)
    col_index = 0
    for i, letter in enumerate(col_letters):
        col_index += (ord(letter.upper()) - ord('A') + 1) * (26 ** (len(col_letters) - i - 1))
    col_index -= 1  # Adjust to 0-based indexing
    
    return col_index

def parse_cell_reference(cell_ref):
    """Parse cell reference like 'A1' into column index and row number"""
    # Find the column letters and row number
    col_letters = ""
    row_number = ""
    
    for char in cell_ref:
        if char.isalpha():
            col_letters += char
        else:
            row_number += char
    
    # Convert column letters to index (A=0, B=1, etc.)
    col_index = parse_column_letter(col_letters)
    
    # Convert row number to integer
    row_num = int(row_number) if row_number else 1
    
    return col_index, row_num

def load_data_with_columns(file_path, coords):
    """Load data from Excel file using specified column ranges"""
    try:
        # Read the Excel file without headers to treat all rows as data
        df = pd.read_excel(file_path, header=None)
        
        print(f"Excel file shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")
        
        # Extract data using column ranges
        def extract_column_range_data(df, col_info):
            if col_info is None:
                return pd.DataFrame()
            
            start_col = col_info['start_col']
            end_col = col_info['end_col']
            
            # Extract the specified column range (all rows)
            data = df.iloc[:, start_col:end_col+1]  # +1 to include end column
            return data
        
        # Extract Dataset 1 data (A to D)
        dataset1_data = extract_column_range_data(df, coords['dataset1'])
        
        # Extract Dataset 2 data (F to I)
        dataset2_data = extract_column_range_data(df, coords['dataset2'])
        
        print(f"\nðŸ“Š Data ranges loaded:")
        print(f"  Dataset1: {coords['dataset1']['range_str']} - {len(dataset1_data)} records")
        print(f"  Dataset2: {coords['dataset2']['range_str']} - {len(dataset2_data)} records")
        
        # Create DataFrames with proper column names
        # Assuming columns are: Name, Position, TIN, Pay
        dataset1 = pd.DataFrame({
            'Name': dataset1_data.iloc[:, 0],      # First column (A)
            'Position': dataset1_data.iloc[:, 1],   # Second column (B)
            'TIN': dataset1_data.iloc[:, 2],        # Third column (C)
            'Pay': dataset1_data.iloc[:, 3]         # Fourth column (D)
        })
        
        dataset2 = pd.DataFrame({
            'Name': dataset2_data.iloc[:, 0],      # First column (F)
            'Position': dataset2_data.iloc[:, 1],   # Second column (G)
            'TIN': dataset2_data.iloc[:, 2],        # Third column (H)
            'Pay': dataset2_data.iloc[:, 3]         # Fourth column (I)
        })
        
        # Store original data before cleaning
        original_dataset1 = dataset1.copy()
        original_dataset2 = dataset2.copy()
        
        # Clean the data
        print(f"\nðŸ” Cleaning data...")
        
        # Remove rows where name is missing or empty
        dataset1 = dataset1.dropna(subset=['Name'])
        dataset2 = dataset2.dropna(subset=['Name'])
        
        # Convert names to string and strip whitespace
        dataset1['Name'] = dataset1['Name'].astype(str).str.strip()
        dataset2['Name'] = dataset2['Name'].astype(str).str.strip()
        
        # Remove empty names
        dataset1 = dataset1[dataset1['Name'] != '']
        dataset2 = dataset2[dataset2['Name'] != '']
        
        # Convert pay to numeric (vectorized)
        dataset1['Pay'] = pd.to_numeric(dataset1['Pay'], errors='coerce')
        dataset2['Pay'] = pd.to_numeric(dataset2['Pay'], errors='coerce')
        
        # Convert positions to string
        dataset1['Position'] = dataset1['Position'].astype(str).str.strip()
        dataset2['Position'] = dataset2['Position'].astype(str).str.strip()
        
        # Convert TIN to string and precompute normalized variants for speed in passes
        dataset1['TIN'] = dataset1['TIN'].astype(str).str.strip()
        dataset2['TIN'] = dataset2['TIN'].astype(str).str.strip()
        
        print(f"  Dataset 1: {len(dataset1)} valid records")
        print(f"  Dataset 2: {len(dataset2)} valid records")
        
        # Find missing records that were filtered out
        missing_records = find_missing_records(original_dataset1, dataset1, "Dataset 1")
        
        return dataset1, dataset2, missing_records
        
    except Exception as e:
        print(f"Error loading data with columns: {e}")
        return pd.DataFrame(), pd.DataFrame(), []

def find_missing_records(original_df, cleaned_df, dataset_name):
    """Find records that were lost during cleaning"""
    missing_records = []
    
    # Convert names to string for comparison
    original_names = original_df['Name'].astype(str).str.strip()
    cleaned_names = cleaned_df['Name'].astype(str).str.strip()
    
    # Find names that were in original but not in cleaned
    for idx, name in enumerate(original_names):
        if name not in cleaned_names.values and name != '' and name != 'nan':
            missing_records.append({
                'index': idx,
                'name': name,
                'position': original_df.iloc[idx]['Position'] if not pd.isna(original_df.iloc[idx]['Position']) else 'Unknown',
                'amount': original_df.iloc[idx]['Pay'] if not pd.isna(original_df.iloc[idx]['Pay']) else 'Unknown',  # Fixed: was 'Amount', now 'Pay'
                'dataset': dataset_name
            })
    
    if missing_records:
        print(f"\nâš ï¸  Found {len(missing_records)} missing records in {dataset_name}:")
        for record in missing_records:
            print(f"    - {record['name']} (Position: {record['position']}, Pay: {record['amount']})")
    
    return missing_records

def load_default_data(file_path):
    """Load data with default column ranges A:D and F:I"""
    # Default column ranges for the new format
    default_coords = {
        'dataset1': {'start_col': 0, 'end_col': 3, 'range_str': 'A:D'},  # A to D
        'dataset2': {'start_col': 5, 'end_col': 8, 'range_str': 'F:I'}   # F to I
    }
    
    return load_data_with_columns(file_path, default_coords)

def clean_name_for_matching(name):
    """Clean and normalize name for more accurate matching (cached)."""
    if pd.isna(name):
        return ""
    return _clean_name_for_matching_cached(str(name))

def extract_name_parts_enhanced(name):
    """Enhanced name parsing with better handling of various formats (cached)."""
    if pd.isna(name):
        return "", "", ""
    return _extract_name_parts_cached(str(name))

def get_enhanced_name_key(name):
    """Create an enhanced name key for more accurate matching"""
    last_name, first_name, middle_part = extract_name_parts_enhanced(name)
    
    # Create multiple possible keys for better matching
    keys = []
    
    # Primary key: Last First
    if last_name and first_name:
        keys.append(f"{last_name} {first_name}")
    
    # Secondary key: Last First Middle
    if last_name and first_name and middle_part:
        keys.append(f"{last_name} {first_name} {middle_part}")
    
    # Tertiary key: Last only (for cases where first name might be different)
    if last_name:
        keys.append(last_name)
    
    # Clean name key (for fuzzy matching)
    clean_name = clean_name_for_matching(name)
    if clean_name:
        keys.append(clean_name)
    
    # Additional normalized keys for special character handling
    # Create normalized versions of the name parts
    normalized_last = clean_name_for_matching(last_name) if last_name else ""
    normalized_first = clean_name_for_matching(first_name) if first_name else ""
    normalized_middle = clean_name_for_matching(middle_part) if middle_part else ""
    
    # Normalized key combinations
    if normalized_last and normalized_first:
        keys.append(f"{normalized_last} {normalized_first}")
    
    if normalized_last and normalized_first and normalized_middle:
        keys.append(f"{normalized_last} {normalized_first} {normalized_middle}")
    
    if normalized_last:
        keys.append(normalized_last)
    
    return keys

def calculate_name_similarity(name1, name2):
    """Calculate similarity between two names"""
    clean1 = clean_name_for_matching(name1)
    clean2 = clean_name_for_matching(name2)
    if not clean1 or not clean2:
        return 0.0
    # Fast cached path
    key_common = sum(1 for c in clean1 if c in clean2)
    similarity = key_common / max(len(clean1), len(clean2))
    if clean1 == clean2:
        return 1.0
    words1 = clean1.split()
    words2 = clean2.split()
    if len(words1) >= 2 and len(words2) >= 2:
        last1, first1 = words1[0], words1[1] if len(words1) > 1 else ""
        last2, first2 = words2[0], words2[1] if len(words2) > 1 else ""
        if last1 == last2 and first1 and first2:
            first_similarity = sum(1 for c in first1 if c in first2) / max(len(first1), len(first2))
            if first_similarity > 0.7:
                return 0.9
    return similarity

def is_same_person_enhanced(name1, name2, position1=None, position2=None):
    """Enhanced function to check if two names are the same person"""
    # Get name keys for both names
    keys1 = get_enhanced_name_key(name1)
    keys2 = get_enhanced_name_key(name2)
    
    # Check for exact key matches
    for key1 in keys1:
        for key2 in keys2:
            if key1 == key2:
                return True
    
    # Check for high similarity
    similarity = calculate_name_similarity(name1, name2)
    if similarity > 0.7:  # More lenient similarity threshold (reduced from 0.8)
        return True
    
    # Check for partial matches (last name + first initial)
    last1, first1, _ = extract_name_parts_enhanced(name1)
    last2, first2, _ = extract_name_parts_enhanced(name2)
    
    if last1 == last2 and first1 and first2 and first1[0] == first2[0]:
        return True
    
    # Additional check for special character variations
    # Compare normalized versions of the names
    clean1 = clean_name_for_matching(name1)
    clean2 = clean_name_for_matching(name2)
    
    if clean1 == clean2:
        return True
    
    # Check if normalized last names match and first names are similar
    if clean1 and clean2:
        words1 = clean1.split()
        words2 = clean2.split()
        
        if len(words1) >= 1 and len(words2) >= 1:
            last1_norm = words1[0]
            last2_norm = words2[0]
            
            if last1_norm == last2_norm:
                # If last names match exactly after normalization, check first names
                if len(words1) > 1 and len(words2) > 1:
                    first1_norm = words1[1]
                    first2_norm = words2[1]
                    
                    # If first names are similar enough, consider them the same person
                    if len(first1_norm) >= 3 and len(first2_norm) >= 3:
                        first_similarity = sum(1 for c in first1_norm if c in first2_norm) / max(len(first1_norm), len(first2_norm))
                        if first_similarity > 0.6:  # More lenient threshold (reduced from 0.7)
                            return True
                else:
                    # If only last names are available and they match, consider them the same
                    return True
    
    return False

def normalize_position(position):
    """Normalize position text for better comparison (cached)."""
    if pd.isna(position):
        return ""
    return _normalize_position_cached(str(position))

@lru_cache(maxsize=500_000)
def _are_positions_similar_cached(norm_pos1: str, norm_pos2: str) -> bool:
    if norm_pos1 == norm_pos2:
        return True
    if norm_pos1 in norm_pos2 or norm_pos2 in norm_pos1:
        return True
    if norm_pos1 and norm_pos2:
        common_chars = sum(1 for c in norm_pos1 if c in norm_pos2)
        similarity = common_chars / max(len(norm_pos1), len(norm_pos2))
        if similarity > 0.7:
            return True
    return False


def are_positions_similar(pos1, pos2):
    """Check if two positions are essentially the same with different formatting"""
    if pd.isna(pos1) or pd.isna(pos2):
        return False
    
    # Normalize both positions
    norm_pos1 = normalize_position(pos1)
    norm_pos2 = normalize_position(pos2)
    return _are_positions_similar_cached(norm_pos1, norm_pos2)

# ----------------------------
# Fast lookup structures for dataset2
# ----------------------------

def _build_dataset2_indices(dataset2: pd.DataFrame) -> dict:
    """Build fast lookup indices for dataset2 to reduce nested scans."""
    indices = {
        'name_upper_to_idx': defaultdict(list),
        'firstlast_to_idx': defaultdict(list),
        'tin_to_idx': defaultdict(list),
        'tinnorm_to_idx': defaultdict(list),
        'pos_upper_to_idx': defaultdict(list),
        'pos_norm_to_idx': defaultdict(list),
        'pay_sorted': [],
        'surname_token_to_idx': defaultdict(list),
        # arrays for fast indexed access
        'names': None,
        'positions': None,
        'positions_norm': None,
        'names_clean': None,
    }

    # Name uppercase mapping and cached arrays
    names_series = dataset2['Name'].astype(str).str.strip()
    name_upper_series = names_series.str.upper()
    for idx, name_u in name_upper_series.items():
        if name_u:
            indices['name_upper_to_idx'][name_u].append(idx)
    indices['names'] = list(names_series)
    indices['names_clean'] = [clean_name_for_matching(n) for n in names_series]

    # First+Last mapping and surname token inverted index
    for idx, name_val in dataset2['Name'].items():
        last, first, _ = extract_name_parts_enhanced(name_val)
        if last and first:
            indices['firstlast_to_idx'][(last.upper(), first.upper())].append(idx)
        try:
            for token in _surname_candidates_cached(str(name_val)):
                indices['surname_token_to_idx'][token].append(idx)
        except Exception:
            pass

    # TIN mappings (raw and normalized)
    for idx, tin_val in dataset2['TIN'].items():
        tin_str = str(tin_val).strip()
        if tin_str and tin_str != 'nan':
            indices['tin_to_idx'][tin_str].append(idx)
            tin_norm = _normalize_tin_value(tin_str)
            if tin_norm:
                indices['tinnorm_to_idx'][tin_norm].append(idx)

    # Position mappings (upper and normalized)
    positions_series = dataset2['Position'].astype(str).str.strip()
    pos_upper_series = positions_series.str.upper()
    for idx, pos_u in pos_upper_series.items():
        if pos_u and pos_u != 'NAN':
            indices['pos_upper_to_idx'][pos_u].append(idx)
    positions_norm = []
    for idx, pos in positions_series.items():
        pos_n = normalize_position(pos)
        positions_norm.append(pos_n)
        if pos_n:
            indices['pos_norm_to_idx'][pos_n].append(idx)
    indices['positions'] = list(positions_series)
    indices['positions_norm'] = positions_norm

    # Pay sorted list for efficient range queries
    pay_vals = dataset2['Pay']
    for idx, p in pay_vals.items():
        if not pd.isna(p) and p > 0:
            indices['pay_sorted'].append((float(p), idx))
    indices['pay_sorted'].sort(key=lambda t: t[0])

    return indices


def _get_pay_range_candidates(pay_sorted: list, center: float, abs_tol: float = None, pct_tol: float = None) -> list:
    """Return indices from pay_sorted within abs or percentage tolerance around center."""
    if center is None or pd.isna(center) or center <= 0 or not pay_sorted:
        return []
    if abs_tol is not None:
        low = center - abs_tol
        high = center + abs_tol
    elif pct_tol is not None:
        low = center * (1 - pct_tol)
        high = center * (1 + pct_tol)
    else:
        return []

    pays = [p for p, _ in pay_sorted]
    left = bisect.bisect_left(pays, low)
    right = bisect.bisect_right(pays, high)
    return [pay_sorted[i][1] for i in range(left, right)]

def individual_verification_passes(dataset1, dataset2):
    """Perform 10 individual verification passes to improve accuracy"""
    
    print(f"\nðŸ” Running 10 individual verification passes...")
    # Optionally build FAISS index for candidate pruning
    build_faiss_name_index(dataset2)
    # Build fast indices for dataset2 to avoid quadratic scans
    d2_indices = _build_dataset2_indices(dataset2)
    
    verification_results = {
        'pass_1_exact_name_match': [],
        'pass_2_first_last_name_match': [],
        'pass_3_tin_exact_match': [],
        'pass_4_tin_format_match': [],
        'pass_5_net_pay_exact_match': [],
        'pass_6_net_pay_close_match': [],
        'pass_7_position_exact_match': [],
        'pass_8_position_normalized_match': [],
        'pass_9_fuzzy_name_match': [],
        'pass_10_comprehensive_score_match': [],
        # New AI-specific passes
        'pass_11_ai_name_semantic': [],
        'pass_12_ai_position_semantic': []
    }
    
    # Worker functions for optional parallel execution
    def _pass1_worker(item):
        idx1, row1 = item
        out = []
        name_u = str(row1['Name']).strip().upper()
        cand_idx2 = d2_indices['name_upper_to_idx'].get(name_u, [])
        for idx2 in cand_idx2:
            row2 = dataset2.loc[idx2]
            base_conf = 0.98
            adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.0)
            out.append({'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                        'name1': row1['Name'], 'name2': row2['Name'],
                        'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                        'match_type': 'Exact Name'})
        return out

    def _pass2_worker(item):
        idx1, row1 = item
        out = []
        last1, first1, _ = extract_name_parts_enhanced(row1['Name'])
        if last1 and first1:
            key = (last1.upper(), first1.upper())
            cand_idx2 = d2_indices['firstlast_to_idx'].get(key, [])
            for idx2 in cand_idx2:
                row2 = dataset2.loc[idx2]
                base_conf = 0.93
                adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.0)
                out.append({'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                            'name1': row1['Name'], 'name2': row2['Name'],
                            'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                            'match_type': 'First+Last Name'})
        return out

    def _pass3_worker(item):
        idx1, row1 = item
        out = []
        tin1 = str(row1['TIN']).strip()
        if tin1 and tin1 != 'nan':
            cand_idx2 = d2_indices['tin_to_idx'].get(tin1, [])
            for idx2 in cand_idx2:
                row2 = dataset2.loc[idx2]
                base_conf = 0.9
                adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.0)
                out.append({'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                            'name1': row1['Name'], 'name2': row2['Name'],
                            'tin1': tin1, 'tin2': str(row2['TIN']).strip(),
                            'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                            'match_type': 'TIN Exact'})
        return out

    def _pass4_worker(item):
        idx1, row1 = item
        out = []
        tin1_norm = _normalize_tin_value(row1['TIN'])
        if tin1_norm and tin1_norm != 'nan' and len(tin1_norm) >= 9:
            cand_idx2 = d2_indices['tinnorm_to_idx'].get(tin1_norm, [])
            for idx2 in cand_idx2:
                row2 = dataset2.loc[idx2]
                base_conf = 0.85
                adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.0)
                out.append({'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                            'name1': row1['Name'], 'name2': row2['Name'],
                            'tin1': row1['TIN'], 'tin2': row2['TIN'],
                            'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                            'match_type': 'TIN Format Match'})
        return out

    def _iter_rows(df):
        return df.iterrows()

    # Pass 1: Exact Name Match (indexed)
    print("  Pass 1: Exact name matching...")
    if _PARALLEL_ENABLED and _Parallel is not None and _delayed is not None:
        jobs = (_delayed(_pass1_worker)(item) for item in _iter_rows(dataset1))
        results_list = _Parallel(n_jobs=_PARALLEL_N_JOBS, backend=_PARALLEL_BACKEND)(jobs)
        for out in results_list:
            verification_results['pass_1_exact_name_match'].extend(out)
    else:
        for item in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass1") if _tqdm else dataset1.iterrows()):
            verification_results['pass_1_exact_name_match'].extend(_pass1_worker(item))
        name_u = str(row1['Name']).strip().upper()
        cand_idx2 = d2_indices['name_upper_to_idx'].get(name_u, [])
        for idx2 in cand_idx2:
            row2 = dataset2.loc[idx2]
            base_conf = 0.98
            adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.0)
            verification_results['pass_1_exact_name_match'].append({
                'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                'name1': row1['Name'], 'name2': row2['Name'],
                'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                'match_type': 'Exact Name'
            })
    
    # Pass 2: First and Last Name Component Match (indexed)
    print("  Pass 2: First and last name component matching...")
    if _PARALLEL_ENABLED and _Parallel is not None and _delayed is not None:
        jobs = (_delayed(_pass2_worker)(item) for item in _iter_rows(dataset1))
        results_list = _Parallel(n_jobs=_PARALLEL_N_JOBS, backend=_PARALLEL_BACKEND)(jobs)
        for out in results_list:
            verification_results['pass_2_first_last_name_match'].extend(out)
    else:
        for item in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass2") if _tqdm else dataset1.iterrows()):
            verification_results['pass_2_first_last_name_match'].extend(_pass2_worker(item))
    
    # Pass 3: TIN Exact Match (indexed)
    print("  Pass 3: TIN exact matching...")
    if _PARALLEL_ENABLED and _Parallel is not None and _delayed is not None:
        jobs = (_delayed(_pass3_worker)(item) for item in _iter_rows(dataset1))
        results_list = _Parallel(n_jobs=_PARALLEL_N_JOBS, backend=_PARALLEL_BACKEND)(jobs)
        for out in results_list:
            verification_results['pass_3_tin_exact_match'].extend(out)
    else:
        for item in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass3") if _tqdm else dataset1.iterrows()):
            verification_results['pass_3_tin_exact_match'].extend(_pass3_worker(item))
    
    # Pass 4: TIN Format-normalized Match (handles dashes, spaces) (indexed)
    print("  Pass 4: TIN format-normalized matching...")
    if _PARALLEL_ENABLED and _Parallel is not None and _delayed is not None:
        jobs = (_delayed(_pass4_worker)(item) for item in _iter_rows(dataset1))
        results_list = _Parallel(n_jobs=_PARALLEL_N_JOBS, backend=_PARALLEL_BACKEND)(jobs)
        for out in results_list:
            verification_results['pass_4_tin_format_match'].extend(out)
    else:
        for item in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass4") if _tqdm else dataset1.iterrows()):
            verification_results['pass_4_tin_format_match'].extend(_pass4_worker(item))
    
    # Pass 5: Net Pay Exact Match (with strict name validation) (indexed)
    print("  Pass 5: Net pay exact matching...")
    for idx1, row1 in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass5") if _tqdm else dataset1.iterrows()):
        pay1 = row1['Pay']
        if not pd.isna(pay1) and pay1 > 0:
            cand_idx2 = _get_pay_range_candidates(d2_indices['pay_sorted'], float(pay1), abs_tol=0.01)
            for idx2 in cand_idx2:
                row2 = dataset2.loc[idx2]
                pay2 = row2['Pay']
                if not pd.isna(pay2) and abs(pay1 - pay2) < 0.01:
                    if names_compatible_strict(row1['Name'], row2['Name']):
                        base_conf = 0.7
                        adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.65)
                        verification_results['pass_5_net_pay_exact_match'].append({
                            'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                            'name1': row1['Name'], 'name2': row2['Name'],
                            'pay1': pay1, 'pay2': pay2,
                            'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                            'match_type': 'Pay Exact + Name Compatible'
                        })

    # Pass 6: Net Pay Close Match (within 5%) (with strict name validation) (indexed)
    print("  Pass 6: Net pay close matching (Â±5%)...")
    for idx1, row1 in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass6") if _tqdm else dataset1.iterrows()):
        pay1 = row1['Pay']
        if not pd.isna(pay1) and pay1 > 0:
            cand_idx2 = _get_pay_range_candidates(d2_indices['pay_sorted'], float(pay1), pct_tol=0.05)
            for idx2 in cand_idx2:
                row2 = dataset2.loc[idx2]
                pay2 = row2['Pay']
                if not pd.isna(pay2) and pay2 > 0:
                    diff_pct = abs(pay1 - pay2) / max(pay1, pay2)
                    if diff_pct <= 0.05:
                        if names_compatible_strict(row1['Name'], row2['Name']):
                            base_conf = 0.6
                            adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.65)
                            verification_results['pass_6_net_pay_close_match'].append({
                                'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                                'name1': row1['Name'], 'name2': row2['Name'],
                                'pay1': pay1, 'pay2': pay2, 'diff_pct': diff_pct,
                                'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                                'match_type': 'Pay Close Match + Name Compatible'
                            })

    # Pass 7: Position Exact Match (with strict name validation) (indexed)
    print("  Pass 7: Position exact matching...")
    for idx1, row1 in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass7") if _tqdm else dataset1.iterrows()):
        pos1 = str(row1['Position']).strip().upper()
        if pos1 and pos1 != 'NAN':
            cand_idx2 = d2_indices['pos_upper_to_idx'].get(pos1, [])
            for idx2 in cand_idx2:
                row2 = dataset2.loc[idx2]
                if names_compatible_strict(row1['Name'], row2['Name']):
                    base_conf = 0.8
                    adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.65)
                    verification_results['pass_7_position_exact_match'].append({
                        'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                        'name1': row1['Name'], 'name2': row2['Name'],
                        'pos1': row1['Position'], 'pos2': row2['Position'],
                        'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                        'match_type': 'Position Exact + Name Compatible'
                    })
    
    # Pass 8: Position Normalized Match (with strict name validation) (indexed)
    print("  Pass 8: Position normalized matching...")
    for idx1, row1 in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass8") if _tqdm else dataset1.iterrows()):
        pos1 = normalize_position(row1['Position'])
        if pos1:
            cand_idx2 = d2_indices['pos_norm_to_idx'].get(pos1, [])
            for idx2 in cand_idx2:
                row2 = dataset2.loc[idx2]
                if names_compatible_strict(row1['Name'], row2['Name']):
                    base_conf = 0.75
                    adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.65)
                    verification_results['pass_8_position_normalized_match'].append({
                        'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                        'name1': row1['Name'], 'name2': row2['Name'],
                        'pos1': row1['Position'], 'pos2': row2['Position'],
                        'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                        'match_type': 'Position Normalized + Name Compatible'
                    })
    
    # Pass 9: Fuzzy Name Match (high similarity)
    print("  Pass 9: Fuzzy name matching...")
    for idx1, row1 in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass9") if _tqdm else dataset1.iterrows()):
        # Prefer FAISS; otherwise restrict candidate set by surname token overlap to reduce O(n^2)
        if _faiss_name_index is not None:
            cand_idx2 = set(faiss_topk_candidates(row1['Name'], k=_FAISS_K))
        else:
            # Candidate set from inverted surname index
            s1_tokens = _surname_candidates_cached(str(row1['Name']))
            cand_idx2 = set()
            if s1_tokens:
                for tok in s1_tokens:
                    cand_idx2.update(d2_indices['surname_token_to_idx'].get(tok, []))
            else:
                cand_idx2 = set(dataset2.index)
        # Secondary pruning: keep only top local name-similar candidates, using cached cleaned names
        if cand_idx2:
            sims = []
            n1c = clean_name_for_matching(row1['Name'])
            names_clean = d2_indices['names_clean']
            for idx2 in cand_idx2:
                n2c = names_clean[idx2]
                # quick char-overlap score (same as calculate_name_similarity core)
                common = sum(1 for c in n1c if c in n2c)
                score = common / max(len(n1c), len(n2c)) if n1c and n2c else 0.0
                sims.append((idx2, score))
            sims.sort(key=lambda t: t[1], reverse=True)
            topN = 80 if _faiss_name_index is not None else 120
            cand_idx2 = [i for i, s in sims[:topN] if s >= 0.5]
        for idx2 in cand_idx2:
            row2 = dataset2.loc[idx2]
            similarity = calculate_name_similarity(row1['Name'], row2['Name'])
            if similarity >= 0.8:  # High similarity threshold
                base_conf = float(similarity)
                adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.5)
                verification_results['pass_9_fuzzy_name_match'].append({
                    'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                    'name1': row1['Name'], 'name2': row2['Name'],
                    'similarity': similarity,
                    'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                    'match_type': 'Fuzzy Name'
                })
    
    # Pass 10: Comprehensive Score Match (candidate pruning)
    print("  Pass 10: Comprehensive scoring match...")
    for idx1, row1 in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass10") if _tqdm else dataset1.iterrows()):
        if _faiss_name_index is not None:
            cand_idx2 = set(faiss_topk_candidates(row1['Name'], k=_FAISS_K))
        else:
            # Use the same inverted surname index as Pass 9
            s1_tokens = _surname_candidates_cached(str(row1['Name']))
            cand_idx2 = set()
            if s1_tokens:
                for tok in s1_tokens:
                    cand_idx2.update(d2_indices['surname_token_to_idx'].get(tok, []))
            else:
                cand_idx2 = set(dataset2.index)
        # Secondary pruning: prefer higher rule-based name similarity first
        if cand_idx2:
            sims = []
            n1c = clean_name_for_matching(row1['Name'])
            names_clean = d2_indices['names_clean']
            for idx2 in cand_idx2:
                n2c = names_clean[idx2]
                common = sum(1 for c in n1c if c in n2c)
                score = common / max(len(n1c), len(n2c)) if n1c and n2c else 0.0
                sims.append((idx2, score))
            sims.sort(key=lambda t: t[1], reverse=True)
            topN = 80 if _faiss_name_index is not None else 120
            cand_idx2 = [i for i, s in sims[:topN] if s >= 0.4]
        for idx2 in cand_idx2:
            row2 = dataset2.loc[idx2]
            score = calculate_comprehensive_match_score(row1, row2)
            if score >= 0.7:  # Comprehensive match threshold
                base_conf = float(score)
                adj_conf, ai_n, ai_p, pos_rule = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.5)
                verification_results['pass_10_comprehensive_score_match'].append({
                    'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                    'name1': row1['Name'], 'name2': row2['Name'],
                    'comprehensive_score': score,
                    'ai_name': ai_n, 'ai_pos': ai_p, 'pos_rule_sim': pos_rule,
                    'match_type': 'Comprehensive'
                })

    # Pass 11: AI Name Semantic Rescue (strong AI name similarity for borderline rule similarity)
    print("  Pass 11: AI name semantic rescue...")
    for idx1, row1 in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass11") if _tqdm else dataset1.iterrows()):
        name1 = row1['Name']
        if _faiss_name_index is not None:
            cand_idx2 = set(faiss_topk_candidates(row1['Name'], k=_FAISS_K))
        else:
            s1_tokens = _surname_candidates_cached(str(name1))
            cand_idx2 = set()
            if s1_tokens:
                for tok in s1_tokens:
                    cand_idx2.update(d2_indices['surname_token_to_idx'].get(tok, []))
            else:
                cand_idx2 = set(dataset2.index)
        # Secondary prune by rule name similarity to shrink AI calls
        if cand_idx2:
            sims = []
            n1c = clean_name_for_matching(name1)
            names_clean = d2_indices['names_clean']
            for idx2 in cand_idx2:
                n2c = names_clean[idx2]
                common = sum(1 for c in n1c if c in n2c)
                score = common / max(len(n1c), len(n2c)) if n1c and n2c else 0.0
                sims.append((idx2, score))
            sims.sort(key=lambda t: t[1], reverse=True)
            topN = 80 if _faiss_name_index is not None else 120
            cand_idx2 = [i for i, s in sims[:topN] if s >= 0.4]
        for idx2 in cand_idx2:
            row2 = dataset2.loc[idx2]
            name2 = row2['Name']
            ai_n, ai_p, pos_rule = get_pair_ai_scores(row1, row2)
            rule_name_sim = advanced_name_similarity(name1, name2)
            if ai_n >= 0.88 and (0.55 <= rule_name_sim < 0.80 or names_compatible_strict(name1, name2)):
                base_conf = 0.70 + 0.30 * ai_n
                adj_conf, ai_n2, ai_p2, pos_rule2 = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.80)
                verification_results['pass_11_ai_name_semantic'].append({
                    'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                    'name1': name1, 'name2': name2,
                    'ai_name': ai_n2, 'ai_pos': ai_p2, 'pos_rule_sim': pos_rule2,
                    'rule_name_sim': rule_name_sim,
                    'match_type': 'AI Name Semantic'
                })

    # Pass 12: AI Position Semantic + Rule Name Gate (confirm matches where titles vary)
    print("  Pass 12: AI position semantic confirmation...")
    for idx1, row1 in (_tqdm(dataset1.iterrows(), total=len(dataset1), desc="Pass12") if _tqdm else dataset1.iterrows()):
        if _faiss_name_index is not None:
            cand_idx2 = set(faiss_topk_candidates(row1['Name'], k=_FAISS_K))
        else:
            s1_tokens = _surname_candidates_cached(str(row1['Name']))
            cand_idx2 = set()
            if s1_tokens:
                for tok in s1_tokens:
                    cand_idx2.update(d2_indices['surname_token_to_idx'].get(tok, []))
            else:
                cand_idx2 = set(dataset2.index)
        # Secondary prune by precomputed normalized positions to reduce AI calls
        if cand_idx2:
            pruned = []
            pos1n = normalize_position(row1['Position'])
            pos2n_list = d2_indices['positions_norm']
            for idx2 in cand_idx2:
                pos2n = pos2n_list[idx2]
                if pos1n == pos2n or (pos1n and pos2n and (pos1n in pos2n or pos2n in pos1n)):
                    pruned.append(idx2)
            cand_idx2 = pruned or list(cand_idx2)
        for idx2 in cand_idx2:
            row2 = dataset2.loc[idx2]
            if not names_compatible_strict(row1['Name'], row2['Name']):
                continue
            ai_n, ai_p, pos_rule = get_pair_ai_scores(row1, row2)
            if ai_p >= 0.85 or pos_rule:
                base_conf = 0.65 + 0.25 * max(ai_p, 0.85 if pos_rule else 0.0)
                adj_conf, ai_n2, ai_p2, pos_rule2 = ai_confidence_boost(row1, row2, base_conf, min_name_gate=0.70)
                verification_results['pass_12_ai_position_semantic'].append({
                    'idx1': idx1, 'idx2': idx2, 'confidence': adj_conf,
                    'name1': row1['Name'], 'name2': row2['Name'],
                    'pos1': row1['Position'], 'pos2': row2['Position'],
                    'ai_name': ai_n2, 'ai_pos': ai_p2, 'pos_rule_sim': pos_rule2,
                    'match_type': 'AI Position Semantic'
                })
    
    # Report verification pass results
    print(f"\nï¿½ Verification Pass Results:")
    for pass_name, matches in verification_results.items():
        pass_num = pass_name.split('_')[1]
        print(f"  Pass {pass_num}: {len(matches)} potential matches")
    
    return verification_results

def calculate_comprehensive_match_score(row1, row2):
    """Calculate a comprehensive match score with strict name-first gating."""
    # Strict gate first (unless strong TIN)
    name1 = row1['Name']
    name2 = row2['Name']
    tin1 = str(row1.get('TIN', '')).strip().replace('-', '').replace(' ', '')
    tin2 = str(row2.get('TIN', '')).strip().replace('-', '').replace(' ', '')
    tin_strong = bool(tin1 and tin2 and tin1 == tin2)
    if not tin_strong and not names_compatible_strict(name1, name2):
        return 0.0

    # Use advanced similarity for the name component + AI name semantic
    name_sim_rule = advanced_name_similarity(name1, name2)
    ai_name, ai_pos, pos_rule_sim = get_pair_ai_scores(row1, row2)
    name_sim = max(name_sim_rule, ai_name)
    
    score = 0.0
    weights = {
        'name': 0.45,
        'tin': 0.25,
        'pay': 0.2,
        'position': 0.1
    }
    
    # Name similarity
    score += name_sim * weights['name']
    
    # TIN match
    tin1 = str(row1['TIN']).strip().replace('-', '').replace(' ', '')
    tin2 = str(row2['TIN']).strip().replace('-', '').replace(' ', '')
    if tin1 and tin2 and tin1 != 'nan' and tin2 != 'nan':
        tin_match = 1.0 if tin1 == tin2 else 0.0
        score += tin_match * weights['tin']
    
    # Pay similarity
    pay1, pay2 = row1['Pay'], row2['Pay']
    if not pd.isna(pay1) and not pd.isna(pay2) and pay1 > 0 and pay2 > 0:
        pay_diff_pct = abs(pay1 - pay2) / max(pay1, pay2)
        pay_sim = max(0, 1 - pay_diff_pct)  # Closer pays get higher scores
        score += pay_sim * weights['pay']
    
    # Position similarity: combine rule-based normalization and AI semantic
    pos_sim_rule = 1.0 if are_positions_similar(row1['Position'], row2['Position']) else 0.0
    pos_sim = max(pos_sim_rule, ai_pos * 0.9)
    score += pos_sim * weights['position']
    
    return score

def consolidate_verification_results(verification_results, dataset1, dataset2):
    """Consolidate results from all verification passes into final matches"""
    print(f"\nðŸ”„ Consolidating verification results...")
    
    # Create a comprehensive match mapping
    potential_matches = {}
    
    # Weight factors for different passes
    pass_weights = {
        'pass_1_exact_name_match': 1.0,
        'pass_2_first_last_name_match': 0.95,
        'pass_3_tin_exact_match': 0.9,
        'pass_4_tin_format_match': 0.85,
        'pass_5_net_pay_exact_match': 0.7,
        'pass_6_net_pay_close_match': 0.6,
        'pass_7_position_exact_match': 0.8,
        'pass_8_position_normalized_match': 0.75,
        'pass_9_fuzzy_name_match': 0.8,
        'pass_10_comprehensive_score_match': 0.7,
        'pass_11_ai_name_semantic': 0.82,
        'pass_12_ai_position_semantic': 0.72
    }
    
    # Collect all potential matches with weighted scores
    for pass_name, matches in verification_results.items():
        weight = pass_weights.get(pass_name, 0.5)
        for match in matches:
            pair_key = (match['idx1'], match['idx2'])
            if pair_key not in potential_matches:
                potential_matches[pair_key] = {
                    'idx1': match['idx1'],
                    'idx2': match['idx2'],
                    'name1': match['name1'],
                    'name2': match['name2'],
                    'total_score': 0.0,
                    'evidence': []
                }
            
            # Add weighted confidence to total score
            confidence = match.get('confidence', 0.5)
            weighted_score = confidence * weight
            potential_matches[pair_key]['total_score'] += weighted_score
            potential_matches[pair_key]['evidence'].append({
                'pass': pass_name,
                'confidence': confidence,
                'weight': weight,
                'weighted_score': weighted_score,
                'match_type': match.get('match_type', 'Unknown')
            })
    
    # Sort by total score and resolve conflicts
    sorted_matches = sorted(potential_matches.values(), key=lambda x: x['total_score'], reverse=True)
    
    # Use greedy matching to avoid duplicate assignments
    used_idx1 = set()
    used_idx2 = set()
    final_matches = []
    
    for match in sorted_matches:
        if match['idx1'] not in used_idx1 and match['idx2'] not in used_idx2:
            if match['total_score'] >= 0.7:  # Minimum confidence threshold
                used_idx1.add(match['idx1'])
                used_idx2.add(match['idx2'])
                final_matches.append(match)
    
    print(f"  Found {len(final_matches)} high-confidence matches from verification passes")
    
    return final_matches

def compare_datasets_enhanced(dataset1, dataset2):
    """Enhanced comparison with AI integration and 10-pass verification"""
    
    print(f"\nðŸ” Enhanced comparison with AI + 10-pass verification system...")
    
    # Initialize AI validation tracking
    ai_validation_results = []
    
    # Load AI model at the beginning
    ai_model = load_ai_model()
    if ai_model:
        print("ðŸ¤– AI model ready for borderline cases (6-8 rule passes)")
    else:
        print("âš ï¸ AI model not available - falling back to rule-based only")
    
    # Step 1: Run individual verification passes
    verification_results = individual_verification_passes(dataset1, dataset2)
    
    # Step 2: Consolidate verification results
    verified_matches = consolidate_verification_results(verification_results, dataset1, dataset2)
    
    # Step 3: Create enhanced name mappings for remaining unmatched records
    dataset1_matches = {}
    dataset2_matches = {}
    
    # Track already matched indices from verification passes
    verified_idx1 = {match['idx1'] for match in verified_matches}
    verified_idx2 = {match['idx2'] for match in verified_matches}
    
    # Process remaining unmatched records from dataset1
    for idx, row in dataset1.iterrows():
        if idx not in verified_idx1:
            keys = get_enhanced_name_key(row['Name'])
            for key in keys:
                if key not in dataset1_matches:
                    dataset1_matches[key] = []
                dataset1_matches[key].append(idx)
    
    # Process remaining unmatched records from dataset2
    for idx, row in dataset2.iterrows():
        if idx not in verified_idx2:
            keys = get_enhanced_name_key(row['Name'])
            for key in keys:
                if key not in dataset2_matches:
                    dataset2_matches[key] = []
                dataset2_matches[key].append(idx)
    
    # Find potential matches from remaining unmatched records
    all_keys = set(dataset1_matches.keys()) | set(dataset2_matches.keys())
    
    matched_pairs = []
    mismatched_amounts = []
    mismatched_positions = []
    mismatched_tins = []
    unmatched_dataset1 = []
    unmatched_dataset2 = []
    
    # Track processed indices (start with verified matches)
    processed_dataset1 = verified_idx1.copy()
    processed_dataset2 = verified_idx2.copy()
    
    # Step 4: Add verified matches to matched_pairs with AI validation
    for verified_match in verified_matches:
        idx1 = verified_match['idx1']
        idx2 = verified_match['idx2']
        
        name1 = dataset1.loc[idx1]['Name']
        name2 = dataset2.loc[idx2]['Name']
        pos1 = dataset1.loc[idx1]['Position']
        pos2 = dataset2.loc[idx2]['Position']
        pay1 = dataset1.loc[idx1]['Pay']
        pay2 = dataset2.loc[idx2]['Pay']
        tin1 = dataset1.loc[idx1]['TIN']
        tin2 = dataset2.loc[idx2]['TIN']
        
        # Apply AI-enhanced decision making
        ai_decision = ai_enhanced_match_decision(dataset1.loc[idx1], dataset2.loc[idx2])
        
        # Create a normalized name for display
        normalized_name = f"VERIFIED: {name1}"
        
        # Add to AI validation results
        ai_validation_results.append({
            'Dataset1_Name': name1,
            'Dataset2_Name': name2,
            'Dataset1_Position': pos1,
            'Dataset2_Position': pos2,
            'Dataset1_Pay': pay1,
            'Dataset2_Pay': pay2,
            'Dataset1_TIN': tin1,
            'Dataset2_TIN': tin2,
            'Match_Status': ai_decision['decision'],
            'Rule_Passes': ai_decision['rule_passes'],
            'AI_Used': ai_decision['ai_used'],
            'Name_Semantic_Score': ai_decision['name_semantic_score'],
            'Position_Semantic_Score': ai_decision['position_semantic_score'],
            'Confidence': ai_decision['confidence'],
            'Verification_Score': verified_match['total_score']
        })
        
        matched_pairs.append({
            'Normalized_Name': normalized_name,
            'Dataset1_Original': name1,
            'Dataset2_Original': name2,
            'Dataset1_Position': pos1,
            'Dataset2_Position': pos2,
            'Dataset1_Pay': pay1,
            'Dataset2_Pay': pay2,
            'Dataset1_TIN': tin1,
            'Dataset2_TIN': tin2,
            'Similarity': verified_match['total_score'],
            'Verification_Evidence': verified_match['evidence'],
            'AI_Decision': ai_decision
        })
        
        # Check for mismatches in verified matches
        # Check for pay mismatches
        pay1_valid = not pd.isna(pay1)
        pay2_valid = not pd.isna(pay2)
        
        if pay1_valid and pay2_valid:
            if abs(pay1 - pay2) > 0.01:
                mismatched_amounts.append({
                    'Name': normalized_name,
                    'Dataset1_Original': name1,
                    'Dataset2_Original': name2,
                    'Dataset1_Position': pos1,
                    'Dataset2_Position': pos2,
                    'Dataset1_Pay': pay1,
                    'Dataset2_Pay': pay2,
                    'Difference': pay1 - pay2,
                    'Similarity': verified_match['total_score'],
                    'Verification_Status': 'VERIFIED',
                    'AI_Decision': ai_decision
                })
        elif pay1_valid != pay2_valid:
            mismatched_amounts.append({
                'Name': normalized_name,
                'Dataset1_Original': name1,
                'Dataset2_Original': name2,
                'Dataset1_Position': pos1,
                'Dataset2_Position': pos2,
                'Dataset1_Pay': pay1,
                'Dataset2_Pay': pay2,
                'Difference': float('nan'),
                'Similarity': verified_match['total_score'],
                'Verification_Status': 'VERIFIED',
                'AI_Decision': ai_decision
            })
        
        # Check for TIN mismatches
        tin1_clean = str(tin1).strip() if not pd.isna(tin1) else ""
        tin2_clean = str(tin2).strip() if not pd.isna(tin2) else ""
        
        if tin1_clean and tin2_clean and tin1_clean != tin2_clean:
            mismatched_tins.append({
                'Name': normalized_name,
                'Dataset1_Original': name1,
                'Dataset2_Original': name2,
                'Dataset1_Position': pos1,
                'Dataset2_Position': pos2,
                'Dataset1_TIN': tin1,
                'Dataset2_TIN': tin2,
                'Similarity': verified_match['total_score'],
                'Verification_Status': 'VERIFIED',
                'AI_Decision': ai_decision
            })
        
        # Check for position mismatches
        if not are_positions_similar(pos1, pos2):
            mismatched_positions.append({
                'Name': normalized_name,
                'Dataset1_Original': name1,
                'Dataset2_Original': name2,
                'Dataset1_Position': pos1,
                'Dataset2_Position': pos2,
                'Dataset1_Normalized': normalize_position(pos1),
                'Dataset2_Normalized': normalize_position(pos2),
                'Similarity': verified_match['total_score'],
                'Verification_Status': 'VERIFIED',
                'AI_Decision': ai_decision
            })
    
    # Step 5: Find additional matches from name-based matching for remaining records
    for key in all_keys:
        dataset1_indices = dataset1_matches.get(key, [])
        dataset2_indices = dataset2_matches.get(key, [])
        
        if dataset1_indices and dataset2_indices:
            # Find the best match among multiple candidates
            best_match = None
            best_similarity = 0
            
            for idx1 in dataset1_indices:
                for idx2 in dataset2_indices:
                    if idx1 in processed_dataset1 or idx2 in processed_dataset2:
                        continue
                    
                    name1 = dataset1.loc[idx1]['Name']
                    name2 = dataset2.loc[idx2]['Name']
                    pos1 = dataset1.loc[idx1]['Position']
                    pos2 = dataset2.loc[idx2]['Position']
                    
                    # Apply AI-enhanced decision making for potential matches
                    ai_decision = ai_enhanced_match_decision(dataset1.loc[idx1], dataset2.loc[idx2])
                    
                    # CRITICAL FIX: Only allow matches if there's basic name compatibility
                    # This prevents matching completely different people based on position alone
                    is_name_compatible = is_same_person_enhanced(name1, name2, pos1, pos2)
                    is_ai_match = ai_decision['decision'].startswith('MATCH')
                    
                    # Reject if AI found them to be different people
                    if ai_decision['decision'] == 'MISMATCH (DIFFERENT_PEOPLE)':
                        continue
                    
                    # Only proceed with match if names are compatible OR AI has strong evidence
                    if is_name_compatible or is_ai_match:
                        similarity = calculate_name_similarity(name1, name2)
                        if similarity > best_similarity:
                            best_similarity = similarity
                            best_match = (idx1, idx2, name1, name2, pos1, pos2, ai_decision)
            
            if best_match:
                idx1, idx2, name1, name2, pos1, pos2, ai_decision = best_match
                processed_dataset1.add(idx1)
                processed_dataset2.add(idx2)
                
                pay1 = dataset1.loc[idx1]['Pay']
                pay2 = dataset2.loc[idx2]['Pay']
                tin1 = dataset1.loc[idx1]['TIN']
                tin2 = dataset2.loc[idx2]['TIN']
                
                # Add to AI validation results
                ai_validation_results.append({
                    'Dataset1_Name': name1,
                    'Dataset2_Name': name2,
                    'Dataset1_Position': pos1,
                    'Dataset2_Position': pos2,
                    'Dataset1_Pay': pay1,
                    'Dataset2_Pay': pay2,
                    'Dataset1_TIN': tin1,
                    'Dataset2_TIN': tin2,
                    'Match_Status': ai_decision['decision'],
                    'Rule_Passes': ai_decision['rule_passes'],
                    'AI_Used': ai_decision['ai_used'],
                    'Name_Semantic_Score': ai_decision['name_semantic_score'],
                    'Position_Semantic_Score': ai_decision['position_semantic_score'],
                    'Confidence': ai_decision['confidence'],
                    'Verification_Score': best_similarity
                })
                
                matched_pairs.append({
                    'Normalized_Name': key,
                    'Dataset1_Original': name1,
                    'Dataset2_Original': name2,
                    'Dataset1_Position': pos1,
                    'Dataset2_Position': pos2,
                    'Dataset1_Pay': pay1,
                    'Dataset2_Pay': pay2,
                    'Dataset1_TIN': tin1,
                    'Dataset2_TIN': tin2,
                    'Similarity': best_similarity,
                    'AI_Decision': ai_decision
                })
                
                # Check for pay mismatches
                pay1_valid = not pd.isna(pay1)
                pay2_valid = not pd.isna(pay2)
                
                if pay1_valid and pay2_valid:
                    if abs(pay1 - pay2) > 0.01:
                        mismatched_amounts.append({
                            'Name': key,
                            'Dataset1_Original': name1,
                            'Dataset2_Original': name2,
                            'Dataset1_Position': pos1,
                            'Dataset2_Position': pos2,
                            'Dataset1_Pay': pay1,
                            'Dataset2_Pay': pay2,
                            'Difference': pay1 - pay2,
                            'Similarity': best_similarity,
                            'AI_Decision': ai_decision
                        })
                elif pay1_valid != pay2_valid:
                    mismatched_amounts.append({
                        'Name': key,
                        'Dataset1_Original': name1,
                        'Dataset2_Original': name2,
                        'Dataset1_Position': pos1,
                        'Dataset2_Position': pos2,
                        'Dataset1_Pay': pay1,
                        'Dataset2_Pay': pay2,
                        'Difference': float('nan'),
                        'Similarity': best_similarity,
                        'AI_Decision': ai_decision
                    })
                
                # Check for TIN mismatches
                tin1_clean = str(tin1).strip() if not pd.isna(tin1) else ""
                tin2_clean = str(tin2).strip() if not pd.isna(tin2) else ""
                
                if tin1_clean and tin2_clean and tin1_clean != tin2_clean:
                    mismatched_tins.append({
                        'Name': key,
                        'Dataset1_Original': name1,
                        'Dataset2_Original': name2,
                        'Dataset1_Position': pos1,
                        'Dataset2_Position': pos2,
                        'Dataset1_TIN': tin1,
                        'Dataset2_TIN': tin2,
                        'Similarity': best_similarity,
                        'AI_Decision': ai_decision
                    })
                
                # Check for position mismatches using enhanced comparison
                if not are_positions_similar(pos1, pos2):
                    mismatched_positions.append({
                        'Name': key,
                        'Dataset1_Original': name1,
                        'Dataset2_Original': name2,
                        'Dataset1_Position': pos1,
                        'Dataset2_Position': pos2,
                        'Dataset1_Normalized': normalize_position(pos1),
                        'Dataset2_Normalized': normalize_position(pos2),
                        'Similarity': best_similarity,
                        'AI_Decision': ai_decision
                    })
    
    # Find unmatched records
    for idx, row in dataset1.iterrows():
        if idx not in processed_dataset1:
            unmatched_dataset1.append({
                'Name': row['Name'],
                'Position': row['Position'],
                'TIN': row['TIN'],
                'Pay': row['Pay']
            })
    
    for idx, row in dataset2.iterrows():
        if idx not in processed_dataset2:
            unmatched_dataset2.append({
                'Name': row['Name'],
                'Position': row['Position'],
                'TIN': row['TIN'],
                'Pay': row['Pay']
            })
    
    # Save AI validation results to CSV
    save_ai_validation_csv(ai_validation_results)
    
    # Print enhanced summary
    ai_matches = len([r for r in ai_validation_results if r['AI_Used']])
    ai_triggered = len([r for r in ai_validation_results if r['AI_Used']])
    
    print(f"\nðŸ“Š AI-Enhanced 10-Pass Verification Summary:")
    print(f"  â€¢ Total matches found: {len(matched_pairs)}")
    print(f"  â€¢ Verified matches: {len(verified_matches)}")
    print(f"  â€¢ Additional name-based matches: {len(matched_pairs) - len(verified_matches)}")
    print(f"  â€¢ AI decisions triggered: {ai_triggered}")
    print(f"  â€¢ TIN mismatches: {len(mismatched_tins)}")
    print(f"  â€¢ Pay mismatches: {len(mismatched_amounts)}")
    print(f"  â€¢ Position mismatches: {len(mismatched_positions)}")
    
    return {
        'matched_pairs': matched_pairs,
        'mismatched_amounts': mismatched_amounts,
        'mismatched_positions': mismatched_positions,
        'mismatched_tins': mismatched_tins,
        'unmatched_dataset1': unmatched_dataset1,
        'unmatched_dataset2': unmatched_dataset2,
        'verification_results': verification_results,
        'ai_validation_results': ai_validation_results
    }

def save_ai_validation_csv(ai_validation_results, output_file="ai_validation_results.csv"):
    """Save AI validation results to CSV file"""
    if not ai_validation_results:
        print("ðŸ“ No AI validation results to save.")
        return
    
    try:
        # Create timestamp for unique filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"ai_validation_results_{timestamp}.csv"
        
        # Define CSV headers
        headers = [
            'Dataset1_Name', 'Dataset2_Name',
            'Dataset1_Position', 'Dataset2_Position', 
            'Dataset1_Pay', 'Dataset2_Pay',
            'Dataset1_TIN', 'Dataset2_TIN',
            'Match_Status', 'Rule_Passes', 'AI_Used',
            'Name_Semantic_Score', 'Position_Semantic_Score',
            'Confidence', 'Verification_Score'
        ]
        
        # Write to CSV
        # Use pandas for faster CSV writing on large datasets
        rows = []
        for result in ai_validation_results:
            row = {}
            for header in headers:
                value = result.get(header, '')
                if value is None:
                    row[header] = ''
                elif isinstance(value, float) and not pd.isna(value):
                    row[header] = float(f"{value:.4f}")
                else:
                    row[header] = value
            rows.append(row)
        pd.DataFrame(rows, columns=headers).to_csv(output_file, index=False, encoding='utf-8')
        
        # Calculate statistics
        total_records = len(ai_validation_results)
        ai_used_count = sum(1 for r in ai_validation_results if r['AI_Used'])
        matches = sum(1 for r in ai_validation_results if r['Match_Status'].startswith('MATCH'))
        ai_matches = sum(1 for r in ai_validation_results if r['Match_Status'] == 'MATCH (AI)')
        ai_mismatches = sum(1 for r in ai_validation_results if r['Match_Status'] == 'MISMATCH (AI)')
        
        print(f"\nðŸ¤– AI Validation Results saved to: {output_file}")
        print(f"   ðŸ“Š Statistics:")
        print(f"   â€¢ Total records processed: {total_records}")
        print(f"   â€¢ AI decisions triggered: {ai_used_count}")
        print(f"   â€¢ Total matches: {matches}")
        print(f"   â€¢ AI-confirmed matches: {ai_matches}")
        print(f"   â€¢ AI-identified mismatches: {ai_mismatches}")
        print(f"   â€¢ Rule-based decisions: {total_records - ai_used_count}")
        
        if ai_used_count > 0:
            avg_name_sim = np.mean([r['Name_Semantic_Score'] for r in ai_validation_results 
                                  if r['AI_Used'] and r['Name_Semantic_Score'] is not None])
            avg_pos_sim = np.mean([r['Position_Semantic_Score'] for r in ai_validation_results 
                                 if r['AI_Used'] and r['Position_Semantic_Score'] is not None])
            print(f"   â€¢ Average name semantic similarity: {avg_name_sim:.3f}")
            print(f"   â€¢ Average position semantic similarity: {avg_pos_sim:.3f}")
        
    except Exception as e:
        print(f"âŒ Error saving AI validation results: {e}")
        import traceback
        traceback.print_exc()

def save_verification_analysis(verification_results, dataset1, dataset2, output_file="verification_analysis.xlsx"):
    """Save detailed verification analysis to Excel"""
    try:
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        
        # Create a new workbook
        wb = Workbook()
        wb.remove(wb.active)
        
        # Summary Sheet
        ws_summary = wb.create_sheet("Verification Summary")
        ws_summary['A1'] = "10-PASS VERIFICATION ANALYSIS SUMMARY"
        ws_summary['A1'].font = Font(bold=True, size=16)
        ws_summary.merge_cells('A1:D1')
        
        summary_data = [
            ["Pass Name", "Total Matches", "Confidence Range", "Description"],
            ["", "", "", ""],
        ]
        
        pass_descriptions = {
            'pass_1_exact_name_match': "Exact name string match",
            'pass_2_first_last_name_match': "First + Last name component match",
            'pass_3_tin_exact_match': "TIN exact match",
            'pass_4_tin_format_match': "TIN format-normalized match",
            'pass_5_net_pay_exact_match': "Net pay exact match",
            'pass_6_net_pay_close_match': "Net pay close match (Â±5%)",
            'pass_7_position_exact_match': "Position exact match",
            'pass_8_position_normalized_match': "Position normalized match",
            'pass_9_fuzzy_name_match': "Fuzzy name similarity match",
            'pass_10_comprehensive_score_match': "Comprehensive scoring match",
            'pass_11_ai_name_semantic': "AI name semantic similarity (rescue borderline cases)",
            'pass_12_ai_position_semantic': "AI position semantic similarity (title normalization)"
        }
        
        for pass_name, matches in verification_results.items():
            pass_num = pass_name.split('_')[1]
            pass_title = f"Pass {pass_num}"
            match_count = len(matches)
            
            if matches:
                confidences = [m.get('confidence', 0) for m in matches]
                conf_range = f"{min(confidences):.3f} - {max(confidences):.3f}"
            else:
                conf_range = "N/A"
            
            description = pass_descriptions.get(pass_name, "Unknown pass")
            summary_data.append([pass_title, match_count, conf_range, description])
        
        # Add summary data to sheet
        for row, (col1, col2, col3, col4) in enumerate(summary_data, start=3):
            ws_summary[f'A{row}'] = col1
            ws_summary[f'B{row}'] = col2
            ws_summary[f'C{row}'] = col3
            ws_summary[f'D{row}'] = col4
            
            if row == 3:  # Header row
                for col in ['A', 'B', 'C', 'D']:
                    cell = ws_summary[f'{col}{row}']
                    cell.font = Font(bold=True)
                    cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                    cell.font = Font(color="FFFFFF", bold=True)
        
        # Create detailed sheets for each pass
        for pass_name, matches in verification_results.items():
            if matches:  # Only create sheets for passes with matches
                pass_num = pass_name.split('_')[1]
                sheet_name = f"Pass {pass_num} Details"
                ws = wb.create_sheet(sheet_name)
                
                # Headers
                headers = ["Name 1", "Name 2", "Confidence", "Match Type", "Additional Info"]
                for col, header in enumerate(headers, 1):
                    cell = ws.cell(row=1, column=col, value=header)
                    cell.font = Font(bold=True)
                    cell.fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
                    cell.font = Font(color="FFFFFF", bold=True)
                
                # Data
                for row, match in enumerate(matches, 2):
                    ws.cell(row=row, column=1, value=match['name1'])
                    ws.cell(row=row, column=2, value=match['name2'])
                    ws.cell(row=row, column=3, value=match.get('confidence', 0))
                    ws.cell(row=row, column=4, value=match.get('match_type', 'Unknown'))
                    
                    # Additional info based on pass type
                    additional_info = ""
                    if 'tin1' in match and 'tin2' in match:
                        additional_info = f"TIN: {match['tin1']} â†” {match['tin2']}"
                    elif 'pay1' in match and 'pay2' in match:
                        additional_info = f"Pay: {match['pay1']} â†” {match['pay2']}"
                    elif 'pos1' in match and 'pos2' in match:
                        additional_info = f"Position: {match['pos1']} â†” {match['pos2']}"
                    elif 'similarity' in match:
                        additional_info = f"Similarity: {match['similarity']:.3f}"
                    
                    ws.cell(row=row, column=5, value=additional_info)
                
                # Auto-adjust column widths
                for column in ws.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    ws.column_dimensions[column_letter].width = adjusted_width
        
        # Auto-adjust summary sheet column widths
        for column in ws_summary.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws_summary.column_dimensions[column_letter].width = adjusted_width
        
        # Save the workbook
        wb.save(output_file)
        print(f"\nðŸ“Š Verification analysis saved to: {output_file}")
        
    except Exception as e:
        print(f"âŒ Error saving verification analysis: {e}")
        import traceback
        traceback.print_exc()

def create_tin_mismatch_report(results, output_file="tin_mismatch_detailed_report.xlsx"):
    """Create a detailed TIN mismatch report with analysis"""
    try:
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        
        mismatched_tins = results.get('mismatched_tins', [])
        
        if not mismatched_tins:
            print("âœ… No TIN mismatches found!")
            return
        
        # Create a new workbook
        wb = Workbook()
        wb.remove(wb.active)
        
        # TIN Mismatch Analysis Sheet
        ws_tin = wb.create_sheet("TIN Mismatch Analysis")
        
        # Headers
        headers = [
            "Name (Dataset1)", "Name (Dataset2)", 
            "TIN (Dataset1)", "TIN (Dataset2)",
            "Position (Dataset1)", "Position (Dataset2)",
            "Pay (Dataset1)", "Pay (Dataset2)",
            "Mismatch Type", "Severity", "Possible Cause", "Recommendation"
        ]
        
        for col, header in enumerate(headers, 1):
            cell = ws_tin.cell(row=1, column=col, value=header)
            cell.font = Font(bold=True)
            cell.fill = PatternFill(start_color="FF6B6B", end_color="FF6B6B", fill_type="solid")
            cell.font = Font(color="FFFFFF", bold=True)
        
        # Analyze each TIN mismatch
        for row, item in enumerate(sorted(mismatched_tins, key=lambda x: x['Name']), 2):
            tin1 = str(item['Dataset1_TIN']).strip()
            tin2 = str(item['Dataset2_TIN']).strip()
            
            # Analyze mismatch type
            tin1_clean = tin1.replace('-', '').replace(' ', '')
            tin2_clean = tin2.replace('-', '').replace(' ', '')
            
            if tin1_clean == tin2_clean:
                mismatch_type = "FORMAT ONLY"
                severity = "LOW"
                cause = "Different TIN formatting (dashes, spaces)"
                recommendation = "Standardize TIN format"
            elif len(tin1_clean) != len(tin2_clean):
                mismatch_type = "LENGTH DIFFERENCE"
                severity = "HIGH"
                cause = "One TIN incomplete or has extra digits"
                recommendation = "Verify correct TIN length and format"
            elif abs(len(tin1_clean) - len(tin2_clean)) <= 2:
                mismatch_type = "SIMILAR LENGTH"
                severity = "MEDIUM"
                cause = "Possible data entry error or version difference"
                recommendation = "Manual verification needed"
            else:
                mismatch_type = "COMPLETELY DIFFERENT"
                severity = "CRITICAL"
                cause = "Different persons or major data error"
                recommendation = "Immediate verification required"
            
            # Check for digit transposition
            if len(tin1_clean) == len(tin2_clean):
                different_positions = sum(1 for a, b in zip(tin1_clean, tin2_clean) if a != b)
                if different_positions <= 2:
                    mismatch_type += " (POSSIBLE TRANSPOSITION)"
                    cause += " or digit transposition"
            
            ws_tin.cell(row=row, column=1, value=item['Dataset1_Original'])
            ws_tin.cell(row=row, column=2, value=item['Dataset2_Original'])
            ws_tin.cell(row=row, column=3, value=tin1)
            ws_tin.cell(row=row, column=4, value=tin2)
            ws_tin.cell(row=row, column=5, value=item['Dataset1_Position'])
            ws_tin.cell(row=row, column=6, value=item['Dataset2_Position'])
            ws_tin.cell(row=row, column=7, value=item.get('Dataset1_Pay', 'N/A'))
            ws_tin.cell(row=row, column=8, value=item.get('Dataset2_Pay', 'N/A'))
            ws_tin.cell(row=row, column=9, value=mismatch_type)
            ws_tin.cell(row=row, column=10, value=severity)
            ws_tin.cell(row=row, column=11, value=cause)
            ws_tin.cell(row=row, column=12, value=recommendation)
            
            # Color code severity
            severity_cell = ws_tin.cell(row=row, column=10)
            if severity == "CRITICAL":
                severity_cell.fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
                severity_cell.font = Font(color="FFFFFF", bold=True)
            elif severity == "HIGH":
                severity_cell.fill = PatternFill(start_color="FF6600", end_color="FF6600", fill_type="solid")
                severity_cell.font = Font(color="FFFFFF", bold=True)
            elif severity == "MEDIUM":
                severity_cell.fill = PatternFill(start_color="FFCC00", end_color="FFCC00", fill_type="solid")
            else:  # LOW
                severity_cell.fill = PatternFill(start_color="90EE90", end_color="90EE90", fill_type="solid")
        
        # Auto-adjust column widths
        for column in ws_tin.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws_tin.column_dimensions[column_letter].width = adjusted_width
        
        # Summary Sheet
        ws_summary = wb.create_sheet("TIN Mismatch Summary")
        
        # Count mismatches by severity
        severity_counts = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0}
        for item in mismatched_tins:
            tin1 = str(item['Dataset1_TIN']).strip().replace('-', '').replace(' ', '')
            tin2 = str(item['Dataset2_TIN']).strip().replace('-', '').replace(' ', '')
            
            if tin1 == tin2:
                severity_counts["LOW"] += 1
            elif abs(len(tin1) - len(tin2)) > 2:
                severity_counts["CRITICAL"] += 1
            elif len(tin1) != len(tin2):
                severity_counts["HIGH"] += 1
            else:
                severity_counts["MEDIUM"] += 1
        
        ws_summary['A1'] = "TIN MISMATCH ANALYSIS SUMMARY"
        ws_summary['A1'].font = Font(bold=True, size=16)
        ws_summary.merge_cells('A1:D1')
        
        summary_data = [
            ["Total TIN Mismatches", len(mismatched_tins)],
            ["", ""],
            ["SEVERITY BREAKDOWN:", ""],
            ["Critical (Completely Different)", severity_counts["CRITICAL"]],
            ["High (Length Difference)", severity_counts["HIGH"]],
            ["Medium (Similar Length)", severity_counts["MEDIUM"]],
            ["Low (Format Only)", severity_counts["LOW"]],
            ["", ""],
            ["RECOMMENDATIONS:", ""],
            ["Critical & High Priority", severity_counts["CRITICAL"] + severity_counts["HIGH"]],
            ["Immediate Review Required", severity_counts["CRITICAL"]],
            ["Format Standardization Needed", severity_counts["LOW"]]
        ]
        
        for i, (label, value) in enumerate(summary_data, start=3):
            ws_summary[f'A{i}'] = label
            ws_summary[f'B{i}'] = value
            if label and not label.startswith(" "):
                ws_summary[f'A{i}'].font = Font(bold=True)
        
        # Save the workbook
        wb.save(output_file)
        print(f"\nðŸ“Š TIN Mismatch Report saved to: {output_file}")
        print(f"   â€¢ Total TIN mismatches: {len(mismatched_tins)}")
        print(f"   â€¢ Critical: {severity_counts['CRITICAL']}")
        print(f"   â€¢ High: {severity_counts['HIGH']}")
        print(f"   â€¢ Medium: {severity_counts['MEDIUM']}")
        print(f"   â€¢ Low: {severity_counts['LOW']}")
        
    except Exception as e:
        print(f"âŒ Error creating TIN mismatch report: {e}")
        import traceback
        traceback.print_exc()

def generate_enhanced_report(results, dataset1, dataset2):
    """Generate an enhanced report with alphabetical sorting"""
    
    print("\n" + "="*80)
    print("ENHANCED EXCEL COMPARISON REPORT")
    print("="*80)
    
    print(f"\nSUMMARY:")
    print(f"Total records in Dataset1 (A-D): {len(dataset1)}")
    print(f"Total records in Dataset2 (F-I): {len(dataset2)}")
    print(f"Matched pairs found: {len(results['matched_pairs'])}")
    
    # Sort all results alphabetically by name
    results['matched_pairs'].sort(key=lambda x: x['Normalized_Name'])
    results['mismatched_amounts'].sort(key=lambda x: x['Name'])
    results.get('mismatched_tins', []).sort(key=lambda x: x['Name'])
    results['mismatched_positions'].sort(key=lambda x: x['Name'])
    results['unmatched_dataset1'].sort(key=lambda x: x['Name'])
    results['unmatched_dataset2'].sort(key=lambda x: x['Name'])
    
    # Helper function to get display name with first name
    def get_display_name(item):
        """Get a display name that includes first name for better identification"""
        if 'Dataset1_Original' in item:
            # For matched pairs, show both original names
            name1 = item['Dataset1_Original']
            name2 = item['Dataset2_Original']
            return f"{name1} â†” {name2}"
        else:
            # For unmatched items, just show the name
            return item['Name']
    
    # Unmatched names in Dataset1 (A-D)
    if results['unmatched_dataset1']:
        print(f"\nâŒ NAMES IN DATASET1 (A-D) BUT NOT MATCHED IN DATASET2 (F-I) ({len(results['unmatched_dataset1'])}):")
        print("-" * 100)
        print(f"{'Name (First Last)':<40} {'Position':<40} {'TIN':<15} {'Pay':<15}")
        print("-" * 100)
        for item in results['unmatched_dataset1']:
            display_name = get_display_name(item)
            print(f"{display_name:<40} {item['Position']:<40} {item['TIN']:<15} {item['Pay']:<15.2f}")
    else:
        print(f"\nâœ… All names from Dataset1 (A-D) were matched in Dataset2 (F-I)")
    
    # Unmatched names in Dataset2 (F-I)
    if results['unmatched_dataset2']:
        print(f"\nâš ï¸  NAMES IN DATASET2 (F-I) BUT NOT MATCHED IN DATASET1 (A-D) ({len(results['unmatched_dataset2'])}):")
        print("-" * 100)
        print(f"{'Name (First Last)':<40} {'Position':<40} {'TIN':<15} {'Pay':<15}")
        print("-" * 100)
        for item in results['unmatched_dataset2']:
            display_name = get_display_name(item)
            print(f"{display_name:<40} {item['Position']:<40} {item['TIN']:<15} {item['Pay']:<15.2f}")
    
    # Mismatched amounts (sorted alphabetically)
    if results['mismatched_amounts']:
        print(f"\nðŸ’° AMOUNT MISMATCHES ({len(results['mismatched_amounts'])}):")
        print("-" * 140)
        print(f"{'Name (First Last)':<35} {'Dataset1 Pay':<15} {'Dataset2 Pay':<15} {'Difference':<15} {'Status':<15} {'Similarity':<10}")
        print("-" * 140)
        for item in results['mismatched_amounts']:
            dataset1_pay = item['Dataset1_Pay']
            dataset2_pay = item['Dataset2_Pay']
            difference = dataset1_pay - dataset2_pay
            
            # Determine status
            if difference > 0:
                status = "DATASET1 HIGHER"
            else:
                status = "DATASET2 HIGHER"
            
            display_name = get_display_name(item)
            print(f"{display_name:<35} {dataset1_pay:<15.2f} {dataset2_pay:<15.2f} {difference:<15.2f} {status:<15} {item['Similarity']:<10.3f}")
    else:
        print(f"\nâœ… All matched names have matching pay amounts")
    
    # Mismatched TINs (sorted alphabetically)
    if results.get('mismatched_tins', []):
        print(f"\nðŸ†” TIN MISMATCHES ({len(results['mismatched_tins'])}):")
        print("-" * 140)
        print(f"{'Name (First Last)':<35} {'Dataset1 TIN':<15} {'Dataset2 TIN':<15} {'Status':<20} {'Similarity':<10}")
        print("-" * 140)
        for item in results['mismatched_tins']:
            tin1 = str(item['Dataset1_TIN']).strip()
            tin2 = str(item['Dataset2_TIN']).strip()
            
            # Determine status
            if len(tin1) != len(tin2):
                status = "DIFFERENT LENGTH"
            elif tin1.replace('-', '') == tin2.replace('-', ''):
                status = "SAME TIN - Different format"
            else:
                status = "DIFFERENT TIN"
            
            display_name = get_display_name(item)
            print(f"{display_name:<35} {tin1:<15} {tin2:<15} {status:<20} {item['Similarity']:<10.3f}")
    else:
        print(f"\nâœ… All matched names have matching TINs")
    
    # Mismatched positions (sorted alphabetically)
    if results['mismatched_positions']:
        print(f"\nðŸ‘” POSITION MISMATCHES ({len(results['mismatched_positions'])}):")
        print("-" * 140)
        print(f"{'Name (First Last)':<30} {'Dataset1 Position':<45} {'Dataset2 Position':<45}")
        print("-" * 140)
        for item in results['mismatched_positions']:
            # Truncate long positions for better display
            pos1 = item['Dataset1_Position'][:40] + "..." if len(item['Dataset1_Position']) > 40 else item['Dataset1_Position']
            pos2 = item['Dataset2_Position'][:40] + "..." if len(item['Dataset2_Position']) > 40 else item['Dataset2_Position']
            display_name = get_display_name(item)
            print(f"{display_name:<30} {pos1:<45} {pos2:<45}")
        
        # Show detailed analysis for each mismatch
        print(f"\nðŸ“‹ DETAILED POSITION ANALYSIS:")
        print("-" * 140)
        for item in results['mismatched_positions']:
            display_name = get_display_name(item)
            print(f"\nðŸ” {display_name}:")
            print(f"   Dataset1: '{item['Dataset1_Position']}'")
            print(f"   Dataset2: '{item['Dataset2_Position']}'")
            print(f"   Normalized1: '{item['Dataset1_Normalized']}'")
            print(f"   Normalized2: '{item['Dataset2_Normalized']}'")
            print(f"   Name Similarity: {item['Similarity']:.3f}")
            
            # Determine if it's a formatting issue or actual different position
            norm1 = item['Dataset1_Normalized']
            norm2 = item['Dataset2_Normalized']
            
            if norm1 == norm2:
                print(f"   âš ï¸  SAME POSITION - Different formatting only")
            elif norm1 in norm2 or norm2 in norm1:
                print(f"   âš ï¸  SIMILAR POSITION - One has extra information")
            else:
                print(f"   âŒ DIFFERENT POSITION - Actual role mismatch")
    else:
        print(f"\nâœ… All matched names have matching positions")
    
    # Perfect matches (sorted alphabetically)
    perfect_matches = [pair for pair in results['matched_pairs'] 
                      if abs(pair['Dataset1_Pay'] - pair['Dataset2_Pay']) <= 0.01 and
                      are_positions_similar(pair['Dataset1_Position'], pair['Dataset2_Position'])]
    
    if perfect_matches:
        print(f"\nâœ… PERFECT MATCHES ({len(perfect_matches)}):")
        print("-" * 140)
        print(f"{'Name (First Last)':<35} {'Position':<50} {'Pay':<15} {'Similarity':<10}")
        print("-" * 140)
        for item in perfect_matches:
            display_name = get_display_name(item)
            print(f"{display_name:<35} {item['Dataset1_Position']:<50} {item['Dataset1_Pay']:<15.2f} {item['Similarity']:<10.3f}")
    
    # Summary statistics
    print(f"\nðŸ“Š SUMMARY STATISTICS:")
    print(f"  â€¢ Total matched pairs: {len(results['matched_pairs'])}")
    print(f"  â€¢ Perfect matches: {len(perfect_matches)}")
    print(f"  â€¢ Names with mismatched pay amounts: {len(results['mismatched_amounts'])}")
    print(f"  â€¢ Names with mismatched TINs: {len(results.get('mismatched_tins', []))}")
    print(f"  â€¢ Names with mismatched positions: {len(results['mismatched_positions'])}")
    print(f"  â€¢ Unmatched in Dataset1 (A-D): {len(results['unmatched_dataset1'])}")
    print(f"  â€¢ Unmatched in Dataset2 (F-I): {len(results['unmatched_dataset2'])}")
    
    return results

def save_enhanced_report(results, missing_records=None, output_file="simpler_comparison_report.txt"):
    """Save enhanced detailed report to file with alphabetical sorting"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("ENHANCED SIMPLER.XLSX COMPARISON REPORT\n")
        f.write("="*80 + "\n\n")
        
        # Add missing records section if any exist
        if missing_records:
            f.write("MISSING RECORDS (EXCLUDED FROM COMPARISON):\n")
            f.write("-" * 100 + "\n")
            f.write("These records were present in the original data but excluded due to missing net pay:\n\n")
            for record in missing_records:
                f.write(f"Dataset: {record['dataset']}\n")
                f.write(f"Name: {record['name']}\n")
                f.write(f"Position: {record['position']}\n")
                f.write(f"Amount: {record['amount']}\n")
                f.write(f"Original Index: {record['index']}\n")
                f.write(f"Reason: Missing Net Pay\n")
                f.write("-" * 50 + "\n")
            f.write("\n")
        
        # Helper function to get display name with first name
        def get_display_name(item):
            """Get a display name that includes first name for better identification"""
            if 'Dataset1_Original' in item:
                # For matched pairs, show both original names
                name1 = item['Dataset1_Original']
                name2 = item['Dataset2_Original']
                return f"{name1} â†” {name2}"
            else:
                # For unmatched items, just show the name
                return item['Name']
        
        f.write("UNMATCHED NAMES IN DATASET1 (A-D):\n")
        f.write("-" * 100 + "\n")
        for item in sorted(results['unmatched_dataset1'], key=lambda x: x['Name']):
            display_name = get_display_name(item)
            f.write(f"{display_name:<40} {item['Position']:<40} {item['TIN']:<15} {item['Pay']:<15.2f}\n")
        
        f.write(f"\nUNMATCHED NAMES IN DATASET2 (F-I):\n")
        f.write("-" * 100 + "\n")
        for item in sorted(results['unmatched_dataset2'], key=lambda x: x['Name']):
            display_name = get_display_name(item)
            f.write(f"{display_name:<40} {item['Position']:<40} {item['TIN']:<15} {item['Pay']:<15.2f}\n")
        
        f.write(f"\nPAY MISMATCHES (Dataset1 â†’ Dataset2):\n")
        f.write("-" * 140 + "\n")
        for item in sorted(results['mismatched_amounts'], key=lambda x: x['Name']):
            dataset1_pay = item['Dataset1_Pay']
            dataset2_pay = item['Dataset2_Pay']
            difference = dataset1_pay - dataset2_pay
            
            if difference > 0:
                status = "DATASET1 HIGHER"
            else:
                status = "DATASET2 HIGHER"
            
            display_name = get_display_name(item)
            f.write(f"{display_name}: Dataset1={dataset1_pay:.2f} â†’ Dataset2={dataset2_pay:.2f} (Diff={difference:.2f}) [{status}] (Similarity={item['Similarity']:.3f})\n")
        
        f.write(f"\nTIN MISMATCHES:\n")
        f.write("-" * 140 + "\n")
        for item in sorted(results.get('mismatched_tins', []), key=lambda x: x['Name']):
            tin1 = str(item['Dataset1_TIN']).strip()
            tin2 = str(item['Dataset2_TIN']).strip()
            
            # Determine status
            if len(tin1) != len(tin2):
                status = "DIFFERENT LENGTH"
            elif tin1.replace('-', '') == tin2.replace('-', ''):
                status = "SAME TIN - Different format"
            else:
                status = "DIFFERENT TIN"
            
            display_name = get_display_name(item)
            f.write(f"{display_name}: Dataset1={tin1} â†’ Dataset2={tin2} [{status}] (Similarity={item['Similarity']:.3f})\n")
        
        f.write(f"\nPOSITION MISMATCHES:\n")
        f.write("-" * 140 + "\n")
        for item in sorted(results['mismatched_positions'], key=lambda x: x['Name']):
            display_name = get_display_name(item)
            f.write(f"\n{display_name}:\n")
            f.write(f"  Dataset1: '{item['Dataset1_Position']}'\n")
            f.write(f"  Dataset2: '{item['Dataset2_Position']}'\n")
            f.write(f"  Normalized1: '{item['Dataset1_Normalized']}'\n")
            f.write(f"  Normalized2: '{item['Dataset2_Normalized']}'\n")
            f.write(f"  Name Similarity: {item['Similarity']:.3f}\n")
            
            # Determine if it's a formatting issue or actual different position
            norm1 = item['Dataset1_Normalized']
            norm2 = item['Dataset2_Normalized']
            
            if norm1 == norm2:
                f.write(f"  STATUS: SAME POSITION - Different formatting only\n")
            elif norm1 in norm2 or norm2 in norm1:
                f.write(f"  STATUS: SIMILAR POSITION - One has extra information\n")
            else:
                f.write(f"  STATUS: DIFFERENT POSITION - Actual role mismatch\n")
        
        f.write(f"\nPERFECT MATCHES:\n")
        f.write("-" * 140 + "\n")
        perfect_matches = [pair for pair in results['matched_pairs'] 
                          if abs(pair['Dataset1_Pay'] - pair['Dataset2_Pay']) <= 0.01 and
                          are_positions_similar(pair['Dataset1_Position'], pair['Dataset2_Position'])]
        
        for item in sorted(perfect_matches, key=lambda x: x['Normalized_Name']):
            display_name = get_display_name(item)
            f.write(f"{display_name}: ({item['Dataset1_Position']}) (Pay: {item['Dataset1_Pay']:.2f}) (Similarity={item['Similarity']:.3f})\n")
    
    print(f"\nðŸ“„ Enhanced detailed report saved to: {output_file}")

def save_excel_report(results, dataset1, dataset2, missing_records=None, output_file="simpler_comparison_report.xlsx"):
    """Save detailed report to Excel file with multiple sheets for better readability"""
    try:
        import pandas as pd
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from openpyxl.utils.dataframe import dataframe_to_rows
        
        # Create a new workbook
        wb = Workbook()
        
        # Remove default sheet
        wb.remove(wb.active)
        
        # Helper function to get display name with first name
        def get_display_name(item):
            """Get a display name that includes first name for better identification"""
            if 'Dataset1_Original' in item:
                # For matched pairs, show both original names
                name1 = item['Dataset1_Original']
                name2 = item['Dataset2_Original']
                return f"{name1} â†” {name2}"
            else:
                # For unmatched items, just show the name
                return item['Name']
        
        # 1. SUMMARY SHEET
        ws_summary = wb.create_sheet("Summary")
        ws_summary['A1'] = "ENHANCED EXCEL COMPARISON REPORT"
        ws_summary['A1'].font = Font(bold=True, size=16)
        ws_summary.merge_cells('A1:F1')
        
        # Summary statistics
        summary_data = [
            ["Total records in Dataset1 (A-D)", len(dataset1)],
            ["Total records in Dataset2 (F-I)", len(dataset2)],
            ["Matched pairs found", len(results['matched_pairs'])],
            ["Perfect matches", len([p for p in results['matched_pairs'] 
                                   if abs(p['Dataset1_Pay'] - p['Dataset2_Pay']) <= 0.01 and
                                   are_positions_similar(p['Dataset1_Position'], p['Dataset2_Position'])])],
            ["Names with mismatched pay amounts", len(results['mismatched_amounts'])],
            ["Names with mismatched TINs", len(results.get('mismatched_tins', []))],
            ["Names with mismatched positions", len(results['mismatched_positions'])],
            ["Unmatched in Dataset1 (A-D)", len(results['unmatched_dataset1'])],
            ["Unmatched in Dataset2 (F-I)", len(results['unmatched_dataset2'])],
            ["Missing Records (Excluded from Comparison)", len(missing_records) if missing_records else 0]
        ]
        
        for i, (label, value) in enumerate(summary_data, start=3):
            ws_summary[f'A{i}'] = label
            ws_summary[f'B{i}'] = value
            ws_summary[f'A{i}'].font = Font(bold=True)
        
        # 2. PERFECT MATCHES SHEET
        ws_perfect = wb.create_sheet("Perfect Matches")
        perfect_matches = [pair for pair in results['matched_pairs'] 
                          if abs(pair['Dataset1_Pay'] - pair['Dataset2_Pay']) <= 0.01 and
                          are_positions_similar(pair['Dataset1_Position'], pair['Dataset2_Position'])]
        
        if perfect_matches:
            # Sort alphabetically
            perfect_matches.sort(key=lambda x: x['Normalized_Name'])
            
            # Headers
            headers = ["Name (First Last)", "Position", "Pay", "Similarity Score"]
            for col, header in enumerate(headers, 1):
                cell = ws_perfect.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(perfect_matches, 2):
                display_name = get_display_name(item)
                ws_perfect.cell(row=row, column=1, value=display_name)
                ws_perfect.cell(row=row, column=2, value=item['Dataset1_Position'])
                ws_perfect.cell(row=row, column=3, value=item['Dataset1_Pay'])
                ws_perfect.cell(row=row, column=4, value=item['Similarity'])
            
            # Auto-adjust column widths
            for column in ws_perfect.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_perfect.column_dimensions[column_letter].width = adjusted_width
        
        # 3. PAY MISMATCHES SHEET
        ws_amounts = wb.create_sheet("Pay Mismatches")
        if results['mismatched_amounts']:
            # Sort alphabetically
            results['mismatched_amounts'].sort(key=lambda x: x['Name'])
            
            # Headers
            headers = ["Name (First Last)", "Dataset1 Pay", "Dataset2 Pay", "Difference", "Status", "Similarity Score"]
            for col, header in enumerate(headers, 1):
                cell = ws_amounts.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="C65911", end_color="C65911", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(results['mismatched_amounts'], 2):
                dataset1_pay = item['Dataset1_Pay']
                dataset2_pay = item['Dataset2_Pay']
                difference = dataset1_pay - dataset2_pay
                
                if difference > 0:
                    status = "DATASET1 HIGHER"
                else:
                    status = "DATASET2 HIGHER"
                
                display_name = get_display_name(item)
                ws_amounts.cell(row=row, column=1, value=display_name)
                ws_amounts.cell(row=row, column=2, value=dataset1_pay)
                ws_amounts.cell(row=row, column=3, value=dataset2_pay)
                ws_amounts.cell(row=row, column=4, value=difference)
                ws_amounts.cell(row=row, column=5, value=status)
                ws_amounts.cell(row=row, column=6, value=item['Similarity'])
            
            # Auto-adjust column widths
            for column in ws_amounts.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_amounts.column_dimensions[column_letter].width = adjusted_width
        
        # 4. TIN MISMATCHES SHEET (NEW)
        ws_tins = wb.create_sheet("TIN Mismatches")
        if results.get('mismatched_tins', []):
            # Sort alphabetically
            results['mismatched_tins'].sort(key=lambda x: x['Name'])
            
            # Headers
            headers = ["Name (First Last)", "Dataset1 TIN", "Dataset2 TIN", "Dataset1 Position", "Dataset2 Position", "Status", "Similarity Score"]
            for col, header in enumerate(headers, 1):
                cell = ws_tins.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="FF6B6B", end_color="FF6B6B", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(results['mismatched_tins'], 2):
                display_name = get_display_name(item)
                
                # Determine status
                tin1 = str(item['Dataset1_TIN']).strip()
                tin2 = str(item['Dataset2_TIN']).strip()
                
                if len(tin1) != len(tin2):
                    status = "DIFFERENT LENGTH - Format issue"
                elif tin1.replace('-', '') == tin2.replace('-', ''):
                    status = "SAME TIN - Different formatting"
                else:
                    status = "DIFFERENT TIN - Actual mismatch"
                
                ws_tins.cell(row=row, column=1, value=display_name)
                ws_tins.cell(row=row, column=2, value=item['Dataset1_TIN'])
                ws_tins.cell(row=row, column=3, value=item['Dataset2_TIN'])
                ws_tins.cell(row=row, column=4, value=item['Dataset1_Position'])
                ws_tins.cell(row=row, column=5, value=item['Dataset2_Position'])
                ws_tins.cell(row=row, column=6, value=status)
                ws_tins.cell(row=row, column=7, value=item['Similarity'])
            
            # Auto-adjust column widths
            for column in ws_tins.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_tins.column_dimensions[column_letter].width = adjusted_width
        
        # 5. POSITION MISMATCHES SHEET
        ws_positions = wb.create_sheet("Position Mismatches")
        if results['mismatched_positions']:
            # Sort alphabetically
            results['mismatched_positions'].sort(key=lambda x: x['Name'])
            
            # Headers
            headers = ["Name (First Last)", "Dataset1 Position", "Dataset2 Position", "Normalized1", "Normalized2", "Status", "Similarity Score"]
            for col, header in enumerate(headers, 1):
                cell = ws_positions.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="70AD47", end_color="70AD47", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(results['mismatched_positions'], 2):
                display_name = get_display_name(item)
                
                # Determine status
                norm1 = item['Dataset1_Normalized']
                norm2 = item['Dataset2_Normalized']
                
                if norm1 == norm2:
                    status = "SAME POSITION - Different formatting"
                elif norm1 in norm2 or norm2 in norm1:
                    status = "SIMILAR POSITION - Extra info"
                else:
                    status = "DIFFERENT POSITION - Actual mismatch"
                
                ws_positions.cell(row=row, column=1, value=display_name)
                ws_positions.cell(row=row, column=2, value=item['Dataset1_Position'])
                ws_positions.cell(row=row, column=3, value=item['Dataset2_Position'])
                ws_positions.cell(row=row, column=4, value=item['Dataset1_Normalized'])
                ws_positions.cell(row=row, column=5, value=item['Dataset2_Normalized'])
                ws_positions.cell(row=row, column=6, value=status)
                ws_positions.cell(row=row, column=7, value=item['Similarity'])
            
            # Auto-adjust column widths
            for column in ws_positions.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_positions.column_dimensions[column_letter].width = adjusted_width
        
        # 6. UNMATCHED DATASET1 SHEET
        ws_unmatched1 = wb.create_sheet("Unmatched Dataset1")
        if results['unmatched_dataset1']:
            # Sort alphabetically
            results['unmatched_dataset1'].sort(key=lambda x: x['Name'])
            
            # Headers
            headers = ["Name (First Last)", "Position", "TIN", "Pay"]
            for col, header in enumerate(headers, 1):
                cell = ws_unmatched1.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(results['unmatched_dataset1'], 2):
                display_name = get_display_name(item)
                ws_unmatched1.cell(row=row, column=1, value=display_name)
                ws_unmatched1.cell(row=row, column=2, value=item['Position'])
                ws_unmatched1.cell(row=row, column=3, value=item['TIN'])
                ws_unmatched1.cell(row=row, column=4, value=item['Pay'])
            
            # Auto-adjust column widths
            for column in ws_unmatched1.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_unmatched1.column_dimensions[column_letter].width = adjusted_width
        
        # 7. UNMATCHED DATASET2 SHEET
        ws_unmatched2 = wb.create_sheet("Unmatched Dataset2")
        if results['unmatched_dataset2']:
            # Sort alphabetically
            results['unmatched_dataset2'].sort(key=lambda x: x['Name'])
            
            # Headers
            headers = ["Name (First Last)", "Position", "TIN", "Pay"]
            for col, header in enumerate(headers, 1):
                cell = ws_unmatched2.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="FF6600", end_color="FF6600", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(results['unmatched_dataset2'], 2):
                display_name = get_display_name(item)
                ws_unmatched2.cell(row=row, column=1, value=display_name)
                ws_unmatched2.cell(row=row, column=2, value=item['Position'])
                ws_unmatched2.cell(row=row, column=3, value=item['TIN'])
                ws_unmatched2.cell(row=row, column=4, value=item['Pay'])
            
            # Auto-adjust column widths
            for column in ws_unmatched2.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_unmatched2.column_dimensions[column_letter].width = adjusted_width
        
        # 8. ALL MATCHES SHEET
        ws_all_matches = wb.create_sheet("All Matches")
        if results['matched_pairs']:
            # Sort alphabetically
            results['matched_pairs'].sort(key=lambda x: x['Normalized_Name'])
            
            # Headers
            headers = ["Name (First Last)", "Dataset1 Position", "Dataset2 Position", "Dataset1 Pay", "Dataset2 Pay", "Pay Match", "Position Match", "Similarity Score"]
            for col, header in enumerate(headers, 1):
                cell = ws_all_matches.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(results['matched_pairs'], 2):
                display_name = get_display_name(item)
                pay_match = "YES" if abs(item['Dataset1_Pay'] - item['Dataset2_Pay']) <= 0.01 else "NO"
                position_match = "YES" if are_positions_similar(item['Dataset1_Position'], item['Dataset2_Position']) else "NO"
                
                ws_all_matches.cell(row=row, column=1, value=display_name)
                ws_all_matches.cell(row=row, column=2, value=item['Dataset1_Position'])
                ws_all_matches.cell(row=row, column=3, value=item['Dataset2_Position'])
                ws_all_matches.cell(row=row, column=4, value=item['Dataset1_Pay'])
                ws_all_matches.cell(row=row, column=5, value=item['Dataset2_Pay'])
                ws_all_matches.cell(row=row, column=6, value=pay_match)
                ws_all_matches.cell(row=row, column=7, value=position_match)
                ws_all_matches.cell(row=row, column=8, value=item['Similarity'])
            
            # Auto-adjust column widths
            for column in ws_all_matches.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_all_matches.column_dimensions[column_letter].width = adjusted_width
        
        # 9. MISSING RECORDS SHEET (NEW)
        if 'missing_records' in locals() and missing_records:
            ws_missing = wb.create_sheet("Missing Records")
            
            # Headers
            headers = ["Dataset", "Name", "Position", "Amount", "Original Index", "Reason"]
            for col, header in enumerate(headers, 1):
                cell = ws_missing.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="C65911", end_color="C65911", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, record in enumerate(missing_records, 2):
                ws_missing.cell(row=row, column=1, value=record['dataset'])
                ws_missing.cell(row=row, column=2, value=record['name'])
                ws_missing.cell(row=row, column=3, value=record['position'])
                ws_missing.cell(row=row, column=4, value=record['amount'])
                ws_missing.cell(row=row, column=5, value=record['index'])
                ws_missing.cell(row=row, column=6, value="Missing Net Pay")
            
            # Auto-adjust column widths
            for column in ws_missing.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_missing.column_dimensions[column_letter].width = adjusted_width
        
        # Save the workbook
        wb.save(output_file)
        print(f"\nðŸ“Š Excel report saved to: {output_file}")
        print(f"   Sheets created:")
        print(f"   â€¢ Summary - Overview statistics")
        print(f"   â€¢ Perfect Matches - Names with matching pay amounts and positions")
        print(f"   â€¢ Pay Mismatches - Names with different pay amounts")
        print(f"   â€¢ TIN Mismatches - Names with different TINs")
        print(f"   â€¢ Position Mismatches - Names with different positions")
        print(f"   â€¢ Unmatched Dataset1 - Names only in first dataset")
        print(f"   â€¢ Unmatched Dataset2 - Names only in second dataset")
        print(f"   â€¢ All Matches - Complete list of all matched pairs")
        if 'missing_records' in locals() and missing_records:
            print(f"   â€¢ Missing Records - Records excluded from comparison")
        
    except ImportError:
        print("âŒ Error: openpyxl not installed. Installing required package...")
        import subprocess
        subprocess.check_call(["pip", "install", "openpyxl"])
        print("âœ… openpyxl installed. Please run the script again.")
    except Exception as e:
        print(f"âŒ Error creating Excel file: {e}")
        print("Falling back to text report...")
        save_enhanced_report(results)

def display_raw_data_comparison(results, dataset1, dataset2, output_file="raw_data_comparison.xlsx"):
    """Display raw data from both datasets side by side for visual confirmation"""
    try:
        import pandas as pd
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        
        # Create a new workbook
        wb = Workbook()
        
        # Remove default sheet
        wb.remove(wb.active)
        
        # 1. PERFECT MATCHES - RAW DATA COMPARISON
        ws_perfect = wb.create_sheet("Perfect Matches - Raw Data")
        perfect_matches = [pair for pair in results['matched_pairs'] 
                          if abs(pair['Dataset1_Pay'] - pair['Dataset2_Pay']) <= 0.01 and
                          are_positions_similar(pair['Dataset1_Position'], pair['Dataset2_Position'])]
        
        if perfect_matches:
            # Sort alphabetically
            perfect_matches.sort(key=lambda x: x['Normalized_Name'])
            
            # Headers
            headers = [
                "Name (Dataset1)", "Position (Dataset1)", "TIN (Dataset1)", "Pay (Dataset1)",
                "Name (Dataset2)", "Position (Dataset2)", "TIN (Dataset2)", "Pay (Dataset2)",
                "Name Match", "Position Match", "Pay Match", "Similarity Score"
            ]
            
            for col, header in enumerate(headers, 1):
                cell = ws_perfect.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(perfect_matches, 2):
                # Determine match status
                name_match = "YES" if item['Dataset1_Original'] == item['Dataset2_Original'] else "SIMILAR"
                position_match = "YES" if are_positions_similar(item['Dataset1_Position'], item['Dataset2_Position']) else "NO"
                
                # Handle pay matching with NaN values
                pay1 = item['Dataset1_Pay']
                pay2 = item['Dataset2_Pay']
                pay1_valid = not pd.isna(pay1)
                pay2_valid = not pd.isna(pay2)
                
                if pay1_valid and pay2_valid:
                    # Both pay amounts are valid numbers
                    pay_match = "YES" if abs(pay1 - pay2) <= 0.01 else "NO"
                elif pay1_valid == pay2_valid:
                    # Both are either valid or both are NaN
                    pay_match = "YES"  # Both missing or both present with same value
                else:
                    # One is valid, the other is NaN
                    pay_match = "NO"
                
                ws_perfect.cell(row=row, column=1, value=item['Dataset1_Original'])
                ws_perfect.cell(row=row, column=2, value=item['Dataset1_Position'])
                ws_perfect.cell(row=row, column=3, value=item['Dataset1_TIN'])
                ws_perfect.cell(row=row, column=4, value=item['Dataset1_Pay'])
                ws_perfect.cell(row=row, column=5, value=item['Dataset2_Original'])
                ws_perfect.cell(row=row, column=6, value=item['Dataset2_Position'])
                ws_perfect.cell(row=row, column=7, value=item['Dataset2_TIN'])
                ws_perfect.cell(row=row, column=8, value=item['Dataset2_Pay'])
                ws_perfect.cell(row=row, column=9, value=name_match)
                ws_perfect.cell(row=row, column=10, value=position_match)
                ws_perfect.cell(row=row, column=11, value=pay_match)
                ws_perfect.cell(row=row, column=12, value=item['Similarity'])
            
            # Auto-adjust column widths
            for column in ws_perfect.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_perfect.column_dimensions[column_letter].width = adjusted_width
        
        # 2. PAY MISMATCHES - RAW DATA COMPARISON
        ws_amounts = wb.create_sheet("Pay Mismatches - Raw Data")
        if results['mismatched_amounts']:
            # Sort alphabetically
            results['mismatched_amounts'].sort(key=lambda x: x['Name'])
            
            # Headers
            headers = [
                "Name (Dataset1)", "Position (Dataset1)", "TIN (Dataset1)", "Pay (Dataset1)",
                "Name (Dataset2)", "Position (Dataset2)", "TIN (Dataset2)", "Pay (Dataset2)",
                "Pay Difference", "Status", "Similarity Score"
            ]
            
            for col, header in enumerate(headers, 1):
                cell = ws_amounts.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="C65911", end_color="C65911", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(results['mismatched_amounts'], 2):
                dataset1_pay = item['Dataset1_Pay']
                dataset2_pay = item['Dataset2_Pay']
                difference = dataset1_pay - dataset2_pay
                
                if difference > 0:
                    status = "DATASET1 HIGHER"
                else:
                    status = "DATASET2 HIGHER"
                
                # Get TIN values - they might not exist in mismatched_amounts
                dataset1_tin = item.get('Dataset1_TIN', 'N/A')
                dataset2_tin = item.get('Dataset2_TIN', 'N/A')
                
                ws_amounts.cell(row=row, column=1, value=item['Dataset1_Original'])
                ws_amounts.cell(row=row, column=2, value=item['Dataset1_Position'])
                ws_amounts.cell(row=row, column=3, value=dataset1_tin)
                ws_amounts.cell(row=row, column=4, value=dataset1_pay)
                ws_amounts.cell(row=row, column=5, value=item['Dataset2_Original'])
                ws_amounts.cell(row=row, column=6, value=item['Dataset2_Position'])
                ws_amounts.cell(row=row, column=7, value=dataset2_tin)
                ws_amounts.cell(row=row, column=8, value=dataset2_pay)
                ws_amounts.cell(row=row, column=9, value=difference)
                ws_amounts.cell(row=row, column=10, value=status)
                ws_amounts.cell(row=row, column=11, value=item['Similarity'])
            
            # Auto-adjust column widths
            for column in ws_amounts.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_amounts.column_dimensions[column_letter].width = adjusted_width
        
        # 3. ALL MATCHES - RAW DATA COMPARISON
        ws_all_matches = wb.create_sheet("All Matches - Raw Data")
        if results['matched_pairs']:
            # Sort alphabetically
            results['matched_pairs'].sort(key=lambda x: x['Normalized_Name'])
            
            # Headers
            headers = [
                "Name (Dataset1)", "Position (Dataset1)", "TIN (Dataset1)", "Pay (Dataset1)",
                "Name (Dataset2)", "Position (Dataset2)", "TIN (Dataset2)", "Pay (Dataset2)",
                "Name Match", "Position Match", "Pay Match", "Similarity Score"
            ]
            
            for col, header in enumerate(headers, 1):
                cell = ws_all_matches.cell(row=1, column=col, value=header)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
                cell.font = Font(color="FFFFFF", bold=True)
            
            # Data
            for row, item in enumerate(results['matched_pairs'], 2):
                # Determine match status
                name_match = "YES" if item['Dataset1_Original'] == item['Dataset2_Original'] else "SIMILAR"
                position_match = "YES" if are_positions_similar(item['Dataset1_Position'], item['Dataset2_Position']) else "NO"
                
                # Handle pay matching with NaN values
                pay1 = item['Dataset1_Pay']
                pay2 = item['Dataset2_Pay']
                pay1_valid = not pd.isna(pay1)
                pay2_valid = not pd.isna(pay2)
                
                if pay1_valid and pay2_valid:
                    # Both pay amounts are valid numbers
                    pay_match = "YES" if abs(pay1 - pay2) <= 0.01 else "NO"
                elif pay1_valid == pay2_valid:
                    # Both are either valid or both are NaN
                    pay_match = "YES"  # Both missing or both present with same value
                else:
                    # One is valid, the other is NaN
                    pay_match = "NO"
                
                ws_all_matches.cell(row=row, column=1, value=item['Dataset1_Original'])
                ws_all_matches.cell(row=row, column=2, value=item['Dataset1_Position'])
                ws_all_matches.cell(row=row, column=3, value=item['Dataset1_TIN'])
                ws_all_matches.cell(row=row, column=4, value=item['Dataset1_Pay'])
                ws_all_matches.cell(row=row, column=5, value=item['Dataset2_Original'])
                ws_all_matches.cell(row=row, column=6, value=item['Dataset2_Position'])
                ws_all_matches.cell(row=row, column=7, value=item['Dataset2_TIN'])
                ws_all_matches.cell(row=row, column=8, value=item['Dataset2_Pay'])
                ws_all_matches.cell(row=row, column=9, value=name_match)
                ws_all_matches.cell(row=row, column=10, value=position_match)
                ws_all_matches.cell(row=row, column=11, value=pay_match)
                ws_all_matches.cell(row=row, column=12, value=item['Similarity'])
            
            # Auto-adjust column widths
            for column in ws_all_matches.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_all_matches.column_dimensions[column_letter].width = adjusted_width
        
        # 4. UNMATCHED DATA - RAW COMPARISON
        ws_unmatched = wb.create_sheet("Unmatched Data - Raw")
        
        # Headers for unmatched data
        headers = [
            "Dataset1 - Name", "Dataset1 - Position", "Dataset1 - TIN", "Dataset1 - Pay",
            "Dataset2 - Name", "Dataset2 - Position", "Dataset2 - TIN", "Dataset2 - Pay"
        ]
        
        for col, header in enumerate(headers, 1):
            cell = ws_unmatched.cell(row=1, column=col, value=header)
            cell.font = Font(bold=True)
            cell.fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
            cell.font = Font(color="FFFFFF", bold=True)
        
        # Combine unmatched data
        max_unmatched = max(len(results['unmatched_dataset1']), len(results['unmatched_dataset2']))
        
        for row in range(max_unmatched):
            row_num = row + 2
            
            # Dataset1 data
            if row < len(results['unmatched_dataset1']):
                item1 = results['unmatched_dataset1'][row]
                ws_unmatched.cell(row=row_num, column=1, value=item1['Name'])
                ws_unmatched.cell(row=row_num, column=2, value=item1['Position'])
                ws_unmatched.cell(row=row_num, column=3, value=item1['TIN'])
                ws_unmatched.cell(row=row_num, column=4, value=item1['Pay'])
            
            # Dataset2 data
            if row < len(results['unmatched_dataset2']):
                item2 = results['unmatched_dataset2'][row]
                ws_unmatched.cell(row=row_num, column=5, value=item2['Name'])
                ws_unmatched.cell(row=row_num, column=6, value=item2['Position'])
                ws_unmatched.cell(row=row_num, column=7, value=item2['TIN'])
                ws_unmatched.cell(row=row_num, column=8, value=item2['Pay'])
        
        # Auto-adjust column widths
        for column in ws_unmatched.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws_unmatched.column_dimensions[column_letter].width = adjusted_width
        
        # Save the workbook
        wb.save(output_file)
        print(f"\nðŸ“Š Raw data comparison saved to: {output_file}")
        print(f"   Sheets created:")
        print(f"   â€¢ Perfect Matches - Raw Data - Side by side comparison of perfect matches")
        print(f"   â€¢ Pay Mismatches - Raw Data - Side by side comparison of pay mismatches")
        print(f"   â€¢ All Matches - Raw Data - Complete side by side comparison of all matches")
        print(f"   â€¢ Unmatched Data - Raw - Side by side view of unmatched records")
        
    except Exception as e:
        print(f"âŒ Error creating raw data comparison: {e}")
        import traceback
        traceback.print_exc()

def run_multiple_comparisons(file_path, num_runs=10):
    """Run the comparison multiple times to ensure accuracy"""
    print(f"\nðŸ”„ Running comparison {num_runs} times to ensure accuracy...")
    
    results_summary = []
    
    for run in range(1, num_runs + 1):
        print(f"\n--- Run {run}/{num_runs} ---")
        
        try:
            # Load data
            dataset1, dataset2, missing_records = load_default_data(file_path)
            
            if dataset1.empty or dataset2.empty:
                print(f"âŒ Run {run}: Failed to load data")
                continue
            
            # Run comparison
            results = compare_datasets_enhanced(dataset1, dataset2)
            
            # Store summary
            summary = {
                'run': run,
                'dataset1_count': len(dataset1),
                'dataset2_count': len(dataset2),
                'matched_pairs': len(results['matched_pairs']),
                'perfect_matches': len([p for p in results['matched_pairs'] 
                                      if abs(p['Dataset1_Pay'] - p['Dataset2_Pay']) <= 0.01 and
                                      are_positions_similar(p['Dataset1_Position'], p['Dataset2_Position'])]),
                'mismatched_amounts': len(results['mismatched_amounts']),
                'mismatched_positions': len(results['mismatched_positions']),
                'unmatched_dataset1': len(results['unmatched_dataset1']),
                'unmatched_dataset2': len(results['unmatched_dataset2'])
            }
            
            results_summary.append(summary)
            print(f"âœ… Run {run}: {summary['matched_pairs']} matches, {summary['perfect_matches']} perfect")
            
        except Exception as e:
            print(f"âŒ Run {run}: Error - {e}")
            continue
    
    # Analyze consistency
    if results_summary:
        print(f"\nðŸ“Š ACCURACY ANALYSIS ({num_runs} runs):")
        print("="*60)
        
        # Check for consistency in key metrics
        matched_counts = [r['matched_pairs'] for r in results_summary]
        perfect_counts = [r['perfect_matches'] for r in results_summary]
        unmatched1_counts = [r['unmatched_dataset1'] for r in results_summary]
        unmatched2_counts = [r['unmatched_dataset2'] for r in results_summary]
        
        print(f"Matched pairs: {min(matched_counts)} - {max(matched_counts)} (range: {max(matched_counts) - min(matched_counts)})")
        print(f"Perfect matches: {min(perfect_counts)} - {max(perfect_counts)} (range: {max(perfect_counts) - min(perfect_counts)})")
        print(f"Unmatched Dataset1: {min(unmatched1_counts)} - {max(unmatched1_counts)} (range: {max(unmatched1_counts) - min(unmatched1_counts)})")
        print(f"Unmatched Dataset2: {min(unmatched2_counts)} - {max(unmatched2_counts)} (range: {max(unmatched2_counts) - min(unmatched2_counts)})")
        
        # Determine consistency
        is_consistent = (max(matched_counts) - min(matched_counts) <= 1 and
                        max(perfect_counts) - min(perfect_counts) <= 1 and
                        max(unmatched1_counts) - min(unmatched1_counts) <= 1 and
                        max(unmatched2_counts) - min(unmatched2_counts) <= 1)
        
        if is_consistent:
            print(f"\nâœ… EXCELLENT: Results are highly consistent across all {num_runs} runs!")
        else:
            print(f"\nâš ï¸  WARNING: Some variation detected across {num_runs} runs")
        
        # Show most common result
        most_common_matched = max(set(matched_counts), key=matched_counts.count)
        most_common_perfect = max(set(perfect_counts), key=perfect_counts.count)
        print(f"\nðŸ“ˆ Most common results:")
        print(f"  â€¢ Matched pairs: {most_common_matched}")
        print(f"  â€¢ Perfect matches: {most_common_perfect}")
    
    return results_summary

def test_special_character_handling():
    """Test function to verify special character handling works correctly"""
    print("\nðŸ§ª Testing special character handling...")
    
    # Test cases for the specific example mentioned
    test_cases = [
        ("Banez, Clarisse Paula D.", "BAÃ‘EZ, CLARISSE PAULA DE GUZMAN"),
        ("Garcia, Juan", "GARCÃA, JUAN"),
        ("MuÃ±oz, Maria", "MUNOZ, MARIA"),
        ("LÃ³pez, Carlos", "LOPEZ, CARLOS"),
        ("GonzÃ¡lez, Ana", "GONZALEZ, ANA"),
        ("JosÃ©, Pedro", "JOSE, PEDRO"),
        ("FranÃ§ois, Pierre", "FRANCOIS, PIERRE"),
        ("MÃ¼ller, Hans", "MULLER, HANS"),
        ("CafÃ©, Jean", "CAFE, JEAN"),
        ("FaÃ§ade, Marie", "FACADE, MARIE")
    ]
    
    print("Testing name matching with special characters:")
    print("-" * 60)
    
    for name1, name2 in test_cases:
        is_match = is_same_person_enhanced(name1, name2)
        similarity = calculate_name_similarity(name1, name2)
        clean1 = clean_name_for_matching(name1)
        clean2 = clean_name_for_matching(name2)
        
        status = "âœ… MATCH" if is_match else "âŒ NO MATCH"
        print(f"{name1:<30} â†” {name2:<30} {status} (Similarity: {similarity:.3f})")
        print(f"  Normalized: '{clean1}' â†” '{clean2}'")
        print()
    
    print("âœ… Special character handling test completed!")

def save_missing_records_sheet(missing_records, output_file="missing_records.xlsx"):
    """Save missing records to a separate Excel sheet"""
    try:
        if not missing_records:
            print("ðŸ“ No missing records to save.")
            return
        
        # Create DataFrame from missing records
        df_missing = pd.DataFrame(missing_records)
        
        # Reorder columns for better readability
        df_missing = df_missing[['dataset', 'name', 'position', 'amount', 'index']]
        df_missing.columns = ['Dataset', 'Name', 'Position', 'Amount', 'Original_Index']
        
        # Save to Excel
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            df_missing.to_excel(writer, sheet_name='Missing_Records', index=False)
            
            # Auto-adjust column widths
            worksheet = writer.sheets['Missing_Records']
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
        
        print(f"ðŸ“„ Missing records saved to: {output_file}")
        print(f"   Total missing records: {len(missing_records)}")
        
    except Exception as e:
        print(f"âŒ Error saving missing records: {e}")
 
def _print_disclaimer_block():
    """Print the module-level disclaimer in a readable block."""
    try:
        text = (__doc__ or "").strip()
    except Exception:
        text = ""
    if not text:
        text = "No disclaimer provided."
    print("\n" + "="*80)
    print("DISCLAIMER")
    print("="*80)
    print(text)
    print("="*80 + "\n")
 
def require_user_agreement():
    """Ask the user to agree to the disclaimer before continuing.
    - Press ESC to decline and exit (Windows consoles).
    - Otherwise, type 'agree' and press Enter to continue.
    """
    _print_disclaimer_block()
    # First, give an immediate ESC-to-exit option on Windows consoles
    try:
        import msvcrt  # Windows-only
        print("Press ESC to decline and exit, or press any other key to proceed to confirmation...")
        key = msvcrt.getch()
        # ESC key
        if key in (b"\x1b", b"\x1B"):
            print("Declined. Exiting...")
            return False
    except Exception:
        # Non-Windows or no console - skip to typed confirmation
        pass
    # Typed confirmation
    resp = input("Type 'agree' to continue: ").strip().lower()
    if resp == "agree":
        return True
    print("Agreement not accepted. Exiting...")
    return False

def main():
    """Main function to run the enhanced comparison with AI integration"""
    
    print("ðŸ” AI-Enhanced Excel Data Comparison Tool")
    print("="*60)
    print("ðŸ“‹ Column Structure: Name, Position, TIN, Pay")
    print("ðŸ“Š Default Ranges: A:D and F:I")
    print("ðŸ¤– AI Model: all-MiniLM-L6-v2 (for borderline cases)")
    
    # Require user to acknowledge the disclaimer before proceeding
    if not require_user_agreement():
        return
    
    # Get file path
    file_path = input("Enter Excel file path (or press Enter for default): ").strip()
    if not file_path:
        file_path = r"C:\Users\Lorenzo Bela\Downloads\Automation\Simpler.xlsx"
    
    print(f"File: {file_path}")
    
    # Check if file exists
    if not Path(file_path).exists():
        print(f"âŒ Error: File not found at {file_path}")
        return
    
    # Ask user if they want to use manual column ranges or default
    print("\nðŸ“‹ Choose analysis type:")
    print("1. AI-Enhanced comparison with default ranges (A:D and F:I) - RECOMMENDED")
    print("2. AI-Enhanced comparison with manual column ranges")
    print("3. Test special character handling")
    print("4. Run multiple comparisons for accuracy testing")
    print("5. TIN-focused analysis (find all TIN mismatches)")
    print("6. Run comprehensive verification analysis")
    print("7. AI Model test (check if model loads correctly)")
    
    choice = input("Enter your choice (1-7): ").strip()
    
    if choice == "3":
        # Test special character handling
        test_special_character_handling()
        return
    elif choice == "4":
        # Run multiple comparisons for accuracy testing
        run_multiple_comparisons(file_path)
        return
    elif choice == "5":
        # TIN-focused analysis
        print("\nðŸ” Running TIN-focused analysis...")
        dataset1, dataset2, missing_records = load_default_data(file_path)
        if not dataset1.empty and not dataset2.empty:
            results = compare_datasets_enhanced(dataset1, dataset2)
            create_tin_mismatch_report(results, "detailed_tin_analysis.xlsx")
            
            # Show quick TIN summary
            tin_mismatches = results.get('mismatched_tins', [])
            print(f"\nðŸ“Š TIN Analysis Results:")
            print(f"  â€¢ Total matches found: {len(results['matched_pairs'])}")
            print(f"  â€¢ TIN mismatches detected: {len(tin_mismatches)}")
            if tin_mismatches:
                print(f"  â€¢ See detailed_tin_analysis.xlsx for complete analysis")
            else:
                print(f"  â€¢ âœ… No TIN mismatches found!")
        return
    elif choice == "6":
        # Comprehensive verification analysis
        print("\nðŸ” Running comprehensive verification analysis...")
        dataset1, dataset2, missing_records = load_default_data(file_path)
        if not dataset1.empty and not dataset2.empty:
            # Run verification passes only
            verification_results = individual_verification_passes(dataset1, dataset2)
            
            print(f"\nðŸ“Š COMPREHENSIVE VERIFICATION ANALYSIS:")
            print(f"="*70)
            
            total_potential_matches = 0
            for pass_name, matches in verification_results.items():
                pass_num = pass_name.split('_')[1]
                pass_desc = pass_name.replace('_', ' ').replace('pass ', 'Pass ').title()
                total_potential_matches += len(matches)
                print(f"  {pass_desc}: {len(matches)} potential matches")
            
            print(f"\n  Total potential matches across all passes: {total_potential_matches}")
            print(f"  Unique records in Dataset1: {len(dataset1)}")
            print(f"  Unique records in Dataset2: {len(dataset2)}")
            
            # Save verification details to Excel
            save_verification_analysis(verification_results, dataset1, dataset2, "verification_analysis.xlsx")
        return
    elif choice == "7":
        # AI Model test
        print("\nðŸ¤– Testing AI model loading...")
        model = load_ai_model()
        if model:
            print("âœ… AI model loaded successfully!")
            # Test semantic similarity with multiple examples
            test_cases = [
                {
                    'name1': "GARCIA, JUAN CARLOS",
                    'name2': "GARCIA, JUAN C.",
                    'pos1': "ADMINISTRATIVE ASSISTANT III",
                    'pos2': "ADMIN ASSISTANT 3"
                },
                {
                    'name1': "SMITH, MARY ELIZABETH",
                    'name2': "SMITH, MARY E.",
                    'pos1': "ENGINEER II",
                    'pos2': "ENG2"
                },
                {
                    'name1': "DELA CRUZ, JOSE",
                    'name2': "DE LA CRUZ, JOSE",
                    'pos1': "INFORMATION SYSTEMS ANALYST",
                    'pos2': "IT SYSTEMS ANALYST"
                }
            ]
            
            print(f"ðŸ§ª AI Decision Test Results:")
            print("="*80)
            
            for i, test_case in enumerate(test_cases, 1):
                name1, name2 = test_case['name1'], test_case['name2']
                pos1, pos2 = test_case['pos1'], test_case['pos2']
                
                # Create mock row objects for testing
                mock_row1 = {
                    'Name': name1, 'Position': pos1, 
                    'TIN': '123-456-789', 'Pay': 50000
                }
                mock_row2 = {
                    'Name': name2, 'Position': pos2,
                    'TIN': '123-456-789', 'Pay': 50000
                }
                
                # Test AI decision
                ai_decision = ai_enhanced_match_decision(mock_row1, mock_row2)
                name_sim = ai_decision['name_semantic_score'] or 0
                pos_sim = ai_decision['position_semantic_score'] or 0
                
                print(f"\nTest {i}:")
                print(f"  Names: '{name1}' vs '{name2}'")
                print(f"  Positions: '{pos1}' vs '{pos2}'")
                print(f"  Name similarity: {name_sim:.3f}")
                print(f"  Position similarity: {pos_sim:.3f}")
                print(f"  Rule passes: {ai_decision['rule_passes']}/10")
                print(f"  Decision: {ai_decision['decision']}")
                if 'explanation' in ai_decision:
                    print(f"  Logic: {ai_decision['explanation']}")
                
                # Show status with emoji
                if ai_decision['decision'].startswith('MATCH'):
                    status = "âœ… MATCH"
                else:
                    status = "âŒ MISMATCH"
                print(f"  Result: {status}")
                
            print("\nðŸŽ¯ Updated AI Thresholds:")
            print("  â€¢ Name similarity: â‰¥ 0.80 (was 0.85)")
            print("  â€¢ Position similarity: â‰¥ 0.75 (was 0.85)")
            print("  â€¢ Hybrid logic: AI name + rule-based position matching")
            print("  â€¢ Fallback: High rule passes + name similarity")
        else:
            print("âŒ AI model failed to load!")
        return
    elif choice == "2":
        # Get manual column ranges
        coords = get_user_columns()
        dataset1, dataset2, missing_records = load_data_with_columns(file_path, coords)
    else:
        # Use default column ranges with AI enhancement
        print("\nðŸ“‚ Loading data with default column ranges...")
        print("ðŸ¤– Initializing AI model for intelligent decision making...")
        dataset1, dataset2, missing_records = load_default_data(file_path)
    
    if dataset1.empty or dataset2.empty:
        print("âŒ Error: Could not load data from the file")
        return
    
    # Compare the data with AI-enhanced matching
    print("\nðŸ” Running AI-enhanced comparison with 10-pass verification...")
    results = compare_datasets_enhanced(dataset1, dataset2)
    
    # Generate and display enhanced report
    results = generate_enhanced_report(results, dataset1, dataset2)
    
    # Save enhanced detailed report
    save_enhanced_report(results, missing_records)
    
    # Save Excel report
    save_excel_report(results, dataset1, dataset2, missing_records)

    # Display raw data comparison
    display_raw_data_comparison(results, dataset1, dataset2)
    
    # Create detailed TIN mismatch report
    create_tin_mismatch_report(results)
    
    # Save missing records to separate sheet
    if missing_records:
        save_missing_records_sheet(missing_records)
    
    # Print detailed verification summary with AI insights
    print(f"\nðŸ“‹ DETAILED VERIFICATION SUMMARY:")
    print(f"="*60)
    verification_results = results.get('verification_results', {})
    for pass_name, matches in verification_results.items():
        pass_num = pass_name.split('_')[1]
        pass_desc = pass_name.replace('_', ' ').replace('pass ', 'Pass ').title()
        print(f"  {pass_desc}: {len(matches)} matches")
        
        # Show top matches for each pass
        if matches and len(matches) <= 5:
            for match in matches[:3]:  # Show first 3 matches
                confidence = match.get('confidence', 0)
                print(f"    â€¢ {match['name1']} â†” {match['name2']} (Confidence: {confidence:.3f})")
        elif matches:
            print(f"    â€¢ Top match: {matches[0]['name1']} â†” {matches[0]['name2']} (Confidence: {matches[0].get('confidence', 0):.3f})")
            print(f"    â€¢ ... and {len(matches)-1} more")
    
    # AI-specific insights
    ai_results = results.get('ai_validation_results', [])
    if ai_results:
        ai_used_count = sum(1 for r in ai_results if r['AI_Used'])
        print(f"\nðŸ¤– AI DECISION INSIGHTS:")
        print(f"  â€¢ Records processed with AI: {ai_used_count}")
        if ai_used_count > 0:
            ai_matches = sum(1 for r in ai_results if r['Match_Status'] == 'MATCH (AI)')
            ai_mismatches = sum(1 for r in ai_results if r['Match_Status'] == 'MISMATCH (AI)')
            print(f"  â€¢ AI-confirmed matches: {ai_matches}")
            print(f"  â€¢ AI-identified mismatches: {ai_mismatches}")
            
            # Show borderline cases that triggered AI
            borderline_cases = [r for r in ai_results if r['AI_Used'] and 6 <= r['Rule_Passes'] <= 8]
            if borderline_cases:
                print(f"  â€¢ Borderline cases (6-8 rule passes): {len(borderline_cases)}")
                for case in borderline_cases[:3]:  # Show first 3
                    name_score = case['Name_Semantic_Score'] or 0
                    pos_score = case['Position_Semantic_Score'] or 0
                    print(f"    â€¢ {case['Dataset1_Name']} â†” {case['Dataset2_Name']}")
                    print(f"      Rules: {case['Rule_Passes']}/10, Name: {name_score:.3f}, Position: {pos_score:.3f} â†’ {case['Match_Status']}")
    
    print(f"\nâœ… AI-Enhanced comparison completed successfully!")
    print(f"\nðŸ“„ Reports Generated:")
    print(f"  â€¢ simpler_comparison_report.txt - Text summary")
    print(f"  â€¢ simpler_comparison_report.xlsx - Main Excel report")
    print(f"  â€¢ raw_data_comparison.xlsx - Side-by-side raw data")
    print(f"  â€¢ tin_mismatch_detailed_report.xlsx - TIN mismatch analysis")
    print(f"  â€¢ ai_validation_results_[timestamp].csv - AI decision details")
    if missing_records:
        print(f"  â€¢ missing_records.xlsx - Excluded records")
    
    # Summary of improvements
    total_matches = len(results['matched_pairs'])
    perfect_matches = len([p for p in results['matched_pairs'] 
                          if abs(p['Dataset1_Pay'] - p['Dataset2_Pay']) <= 0.01 and
                          are_positions_similar(p['Dataset1_Position'], p['Dataset2_Position'])])
    
    print(f"\nðŸŽ¯ ACCURACY METRICS:")
    print(f"  â€¢ Total Records Matched: {total_matches}")
    print(f"  â€¢ Perfect Matches: {perfect_matches}")
    print(f"  â€¢ Match Rate: {(total_matches / max(len(dataset1), len(dataset2)) * 100):.1f}%")
    print(f"  â€¢ Perfect Match Rate: {(perfect_matches / total_matches * 100):.1f}%")
    
    if results.get('mismatched_tins'):
        print(f"  â€¢ TIN Issues Found: {len(results['mismatched_tins'])} (See detailed report)")
    
    return results

if __name__ == "__main__":
    main() 